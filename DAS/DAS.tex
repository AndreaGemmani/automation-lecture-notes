
\documentclass{book}
\usepackage{amsmath, amsthm, graphicx, amsfonts, float, bm}
\usepackage[english]{babel}
\graphicspath{ {./images/} }

\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,237mm},
 left=20mm,
 top=30mm,
 }
 \usepackage[hidelinks]{hyperref}

\newcommand\at[2]{\left.#1\right|_{#2}}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\des}{des}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\notimplies}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\deriv}[1]{\displaystyle\frac{d}{d #1}}
\newcommand{\traj}{(\bar{\mathbf{x}},\bar{\mathbf{u}})}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{remark}
\newtheorem*{notation}{Notation}
\theoremstyle{definition}
\newtheorem*{lemma}{Lemma}

\title{Distributed Autonomous Systems M}
\author{Dante Piotto}
\date{spring semester 2024}


\begin{document}
\maketitle
\tableofcontents

\chapter{Introduction and scenarios}
\section{Distributed Autonomous System}
Each agent $i\in\{1,\dots,N\}$ has 
\begin{itemize}
    \item local physical and/or cyber state $x_i$
    \item computational and sensing capabilities
    \item communication capability: exchange messages with "neighbours"
\end{itemize}


\section{Scenarios and applications of distributed systems}
\begin{itemize}
    \item Averaging: distributed estimation, opinion dynamics
    \item Distrbuted control in cooperative robotics
    \item Distributed optimization 
        \begin{itemize}
            \item distributed machine learning
            \item distributed decision-making in cooperative robotics
            \item distributed optimal control in energy systems and cooperative robotics
        \end{itemize}
\end{itemize}


\section{Measurement filtering in wireless sensor networks}
Consider a network of $N$ sensors with local sensing, computation and communication. Agent $i,i\in\{1,\dots,N\}$, takes a local measurement from the environment (temperature, pressure, etc.). Let $x_{i0}\in\R$ be the scalar local measurement
Agents are interested in agreeing on the average of the measurements, 
\[
    x_{\text{avg}}=\displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}x_{i0}
\]
to have a better estimate of the environment quantity

Consider the following "distributed algorithm" based on "local" linear averaging, for each $i\in\{1,\dots,N\}$
\begin{gather*}
    x_i^0=x_{i0}\\
    x_i^{k+1} = \text{average}(x_i^k,\{x_j^k,j\text{ "neighbour" of }i\}), \qquad k\in\N
\end{gather*}
generalizing coefficients of the update: 
\begin{gather*}
    x_i^0=x_{i0}\\
    x_i^{k+1} = \displaystyle\sum_{j=1}^{N} a_{ij}x_j^k \qquad k\in\N
\end{gather*}
\begin{remark}
    $a_{ij}\geq 0$ and $\sum_{j=1}^{N}a_{ij}=1$
\end{remark}
\begin{remark}
    $a_{ij}=0$, for some $j\in\{1,\dots,N\}$, i.e. $a_{ij}=0$ if $i$ does not have access to the estimate of $j$
\end{remark}


\section{Parameter Estimation in Wireless Sensor Networks}

Consider a network of $N$ sensors with local sensing, computation and communication aiming at estimating a common parameter $\theta^*\in\R$
Each sensor $i$ measures 
\[
    y_i=B_i\theta^* + v_i
\]
with $y_i\in\R^{m_1},B_i$ known matrix and $v_i$ a random measurement noise. Assume $v_1,\dots,v_N$ independent and Gaussian, with zero mean and covariance $E[v_iv_i^T]=\Sigma_i$. Assume $\sum_{i=1}^{N}m_i\geq m$ and $\begin{bmatrix}
    B_1 \\ \vdots \\ B_N
\end{bmatrix} $ full rank
Compute a least-squares estimate 
\[
    \hat\theta = \argmin_\theta \displaystyle\sum_{i=1}^{N}(y_i-B_i\theta)^T\Sigma_i^{-1}(y_i-B_i\theta)
\]
The optimal solution is 
\begin{align*}
    \hat\theta &= \left(\displaystyle\sum_{i=1}^{N}B_i^T\Sigma_i^{-1}B_i \right)^{-1} \displaystyle\sum_{i=1}^{N}B_i^T\Sigma_i^{-1}y_i\\ 
    & =  \left(\displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}B_i^T\Sigma_i^{-1}B_i \right)^{-1} \displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}B_i^T\Sigma_i^{-1}y_i\\ 
\end{align*}
The optimal solution can be obtained by computing two averages $\frac{1}{N}\displaystyle\sum_{i=1}^{N}\beta_i$ and $\frac{1}{N}\displaystyle\sum_{i=1}^{N}\beta_i$

\subsection{Opinion Dynamics in Social Influence Networks}
Group of $N$ individuals, with $x_i^k$ being the opinion of individual $i$ at time $k$. Opinions are updated according to 
\[
    x_i^{k+1} = \displaystyle\sum_{j=1}^{N}a_{ij}x_j^k
\]


\section{Main questions in averaging algorithms}
\begin{itemize}
    \item Do node estimates converge? Do they converge to a common value ("reach consensus")?
    \item Do they reach consensus to the average ("average consensus")?
    \item How can we model communication  in general networks?
    \item Can we answer the above questions for general networks and communication protocols?
    \item What assumptions do we need on the communication network?
\end{itemize}

\section{Distributed control in cooperative robotics}
Team of $N$ (mobile) robots aiming to execute complex tasks
\subsubsection{Basic tasks}
\begin{itemize}
    \item rendevous, containment
    \item formation, flocking
    \item coverage
\end{itemize}
\subsubsection{Complex tasks}
\begin{itemize}
    \item pickup and delivery
    \item surveillance and patrolling
    \item exploration
    \item satellite constellation
\end{itemize}
\subsection{Main questions in cooperative robotics}
\begin{itemize}
    \item Do robot states asymptotically converge? 
    \item Do the asympototic staes satisfy the global, desired task?
    \item How can we model communication in (general) robotic networks?
    \item What assumptions do we need on the communication network?
    \item Can we answer the above questions for general networks and communication protocols?
\end{itemize}
\subsection{Distributed optimal control}
\begin{align*}
    \min_{\substack{x_1,\dots,x_N \\ u_1,\dots,u_N}} & \displaystyle\sum_{i=1}^{N}(\displaystyle\sum_{\tau=0}^{T-1}\ell_i(z_{i,\tau},u_{i,\tau})+m_i(z_{i,T}))\\
    \text{subj to} & \displaystyle\sum_{i=1}^{N}H_iz_{i,\tau}\leq h, && \tau\in[0,T]\\ 
    & z_{i,\tau+1}=A_iz_{i,\tau}+B_iu_{i,\tau} && \forall i, \tau \in [0,T]\\ 
    & z_{i,\tau}\in Z_i, \quad u_{i,\tau}\in U_i, && \forall i, \tau \in [0,T]\\ 
\end{align*}









\chapter{Preliminaries on Algebraic Graph Theory}
\begin{definition}[Digraph]
    A digraph is a pair $G=(I,E)$ where $I={1,\dots,N}$ is a set of elements called \emph{nodes} and $E\subset I \times I$ is a set of ordered node pairs called \emph{edges}\\
    \emph{Edge}: the pair $(i,j)$ denotes an edge from $i$ to $j$\\ 
    \emph{Self-loop}: edge from a node to itself, i.e. $(i,i)$
\end{definition}
\begin{definition}[Undirected (di)graph]
    if for any $(i,j)\in E$ then $(j,i)\in E$  
\end{definition}
\begin{definition}[Subgraph]
    $(I',E')$ subgraph of $(I,E)$ if $I'\subset I$ and $E' \subset E$. Spanning subgraph if $I'=I$
\end{definition}
\begin{definition}[In-neighbours of $i$]$j\in I$ is an in-neighbour of $i\in I$ if $(j,i)\in E$
\end{definition}
\begin{definition}[Set of in-neighbours of $i$]
    $\mathcal{N}_i^{\text{IN}}=\{j\in\{1,\dots,N\}|(j,i)\in E\}$
\end{definition}
\begin{definition}[Out-neighbours of $i$]$j\in I$ is an out-neighbour of $i\in I$ if $(i,j)\in E$
\end{definition}
\begin{definition}[Set of out-neighbours of $i$]
    $\mathcal{N}_i^{\text{IN}}=\{j\in\{1,\dots,N\}|(i,j)\in E\}$
\end{definition}
\begin{definition}[In-degree $\deg_i^{\text{IN}}$]
    number of in-neighbours, i.e. carinality of $\mathcal{N}_i^{\text{IN}}(\deg_i^{\text{IN}}=|\mathcal{N}_i^{\text{IN}}|)$

    Out-degree analogous
\end{definition}
\begin{definition}[Balanced digraph]
    A digraph $G$ is balanced if $\deg_i^{\text{IN}}=\deg_i^{\text{OUT}}$ for all $i\in\{1,\dots,N\}$
\end{definition}
% TODO definitions slides 4-5-7-8-9-10

\begin{definition}[Complete graph]
    Unweighted graph such that $\forall i,j\ \exists \ (i,j),\ (j,i) \in E$
\end{definition}

\chapter{Averaging Systems}
\section{Distributed algorithm}
Given a network of $N$ agents communicating according to a fixed digraph $G$, i.e. each agent $i$ can receive messages only from in-neighbours in the graph, i.e. from $j\in\mathcal{N}_i^{\text{IN}}$. 
We start by considering a fixed graph, thus, each agent communicates with the same neighbours at each iteration $k\in\N$
\[
    x_i^{k+1}=\text{stf}_i(x_i^k,\{x_j^k\}_{j\in\mathcal{N}_i^{\text{IN}}}), \qquad i\in\{1,\dots,N\}
\]
where $\text{stf}_i$ is a function depending only on state $x_i$ and states $x_j,j\in\mathcal{N}_i^{\text{IN}}$.

Alternative version with out-neighbours:
\[
    x_i^{k+1}=\text{stf}_i(\{x_j\}_{j\in\mathcal{N}_i^{\text{OUT}}})
\]
\section{Discrete-time averaging systems}
Let $G^{\text{comm}}=(I,E)$ be a fixed (communication) digraph (self loops included). A linear averaging distributed algorithm can be written as:
\[
    x_i^{k+1}=\displaystyle\sum_{J\in\mathcal{N}_i^{\text{IN}}}a_{ij}x_j^k \qquad i\in\{1\dots,N\}
\]
where $x_i^k\in\R$ is the state of agent $i$ at $k$ and $a_{ij}>0$ are positive weights. 
\begin{remark}
    The weights $a_{ij}$ are defined only for $(i,j)\in E$
\end{remark}
Wach $i$ uses only the states of neighbours $j\in\mathcal{N}_i^{\text{IN}}$, thus distributed algorithm.

For analysis purposes, let us define weights $a_{ij}=0$ for $(j,i)\notin E$. Thus we can rewrite the distributed algorithm as 
\[
    x_i^{k+1}=\displaystyle\sum_{j=1}^{N}a_{ij}x_j^k \qquad i\in\{1,\dots,N\}
\]
This is a LTI autonomous system 
\[
    \begin{bmatrix}
        x_1^{k+1}\\ \vdots \\ x_N^{k+1}
    \end{bmatrix} = \begin{bmatrix}
        a_{11} & \cdots & a_{1N} \\ 
        \vdots & \ddots & \vdots \\
        a_{N1} & \cdots & a_{NN}
    \end{bmatrix} \begin{bmatrix}
        x_1^k \\ \vdots \\ x_N^k
    \end{bmatrix}
\]
Which can be compactly written as  
\[
    x^{k+1} 0 Ax^k
\]
\begin{remark}
    The matrix A can be seen as the weighted adjacency matrix of the reverse digraph $G^{\text{comm,rev}}$ of the digraph $G^{\text{comm}}$
\end{remark}
If instead of in-neighbours we use out-neighbours, we call the digraph a sensing digraph $G^{\text{sens}}$. In this case the notation becomes consistent with graph theory, so we get 
\[
    x^{k+1} = A x^k
\]
where $A$ can be seen as the weighted adjacency matrix of the sensing digraph $G^{\text{sens}}$

\section{Stochastic matrices}
The non-negative square matrix $A\in\R^{N\times N}$ is 
\begin{itemize}
    \item row stochastic if $A\mathbf{1}=\mathbf{1}$ (each row sums to 1)
    \item column stochastic if $A^\top \mathbf{1}=\mathbf{1}$ (each column sums to 1)
    \item doubly stochastic if both row and column stochastic.
\end{itemize}
\begin{lemma}
    Let $A$ be a row-stochastic matrix and $G$ the associate digraph. If $G$ is strongly connected and aperiodic, then 
    \begin{enumerate}
        \item the eigenvalue $\lambda=1$ is simple; 
        \item all the other eigenvalues $\mu$ satisfy $|\mu|<1$
    \end{enumerate}
\end{lemma}
\begin{remark}
    The condition "$G$ contains a globally reachable node and the subgraph of globally reachable noes is aperiodic" is necessary and sufficient
\end{remark}
\begin{theorem}[Consensus]
    Consider a (discrete-time) averaging system with associated digraph $G$ and wieghted adjacency matrix $A$. Assume $G$ is strongly connected and aperiodic, and $A$ is row stochastic. Then 
    \begin{enumerate}
        \item there exists a left eigenvector $w\in\R^N,w>0$ (i.e. with positive components $w_i>0$ for all $i=1,\dots,N$) such that 
            \[
                \lim_{k\to\infty}x^k = \mathbf{1}\displaystyle\frac{w^\top x^0}{w^\top \mathbf{1}} = \begin{bmatrix}
                    1 \\ \vdots \\ 1
                \end{bmatrix} \displaystyle\frac{\sum_{i=1}^{N}w_ix_i^0}{\sum_{i=1}^{N}w_i}
            \]
            i.e., consensus is reached to $ \displaystyle\frac{\sum_{i=1}^{N}w_ix_i^0}{\sum_{i=1}^{N}w_i}$
        \item if additionally $A$ is doubly stochastic, then 
            \[
                \lim_{k\to\infty}x^k = \begin{bmatrix}
                    1 \\ \vdots \\ 1
                \end{bmatrix} \displaystyle\frac{\sum_{i=1}^{N}x_i^0}{N}
            \]
            i.e., average consensus is reached
    \end{enumerate}
\end{theorem}

\section{Example: Metropolis-Hastings weights}
Given an undirected unweighted graph $G$ with edge set $E$ and degrees $d_1,\dots,d_n$
\[
    a_{ij} = \begin{cases}
        \displaystyle\frac{1}{1+\max\{d_i,d_j\}} & \text{if } (i,j)\in E \text{ and } i\neq j\\
        1- \displaystyle\sum_{h\in\mathcal{N}_i \backslash \{i\}}^{} a_{ih} & \text{if } i=j\\ 
        0 & \text{otherwise}
    \end{cases}
\]
Result: the matrix $A$ is symmetric and doubly-stochastic.

























\end{document}
