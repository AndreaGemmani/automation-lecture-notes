
\documentclass{book}
\usepackage{amsmath, amsthm, graphicx, amsfonts, float, bm}
\usepackage[english]{babel}
\graphicspath{ {./images/} }

\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,237mm},
 left=20mm,
 top=30mm,
 }
 \usepackage[hidelinks]{hyperref}

\newcommand\at[2]{\left.#1\right|_{#2}}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\des}{des}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\notimplies}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\deriv}[1]{\displaystyle\frac{d}{d #1}}
\newcommand{\traj}{(\bar{\mathbf{x}},\bar{\mathbf{u}})}


\newtheoremstyle{theoremv2}{}{}{}{}{\bfseries}{}{\newline}{}
\theoremstyle{theoremv2}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheoremstyle{defv2}{}{}{}{}{\bfseries}{}{\newline}{}
\theoremstyle{defv2}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{remark}
\newtheorem*{notation}{Notation}
\theoremstyle{definition}
\newtheorem*{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem*{corollary}{Corollary}

\title{Distributed Autonomous Systems M}
\author{Dante Piotto}
\date{spring semester 2024}


\begin{document}
\maketitle
\tableofcontents

\chapter{Introduction and scenarios}
\section{Distributed Autonomous System}
Each agent $i\in\{1,\dots,N\}$ has 
\begin{itemize}
    \item local physical and/or cyber state $x_i$
    \item computational and sensing capabilities
    \item communication capability: exchange messages with "neighbours"
\end{itemize}


\section{Scenarios and applications of distributed systems}
\begin{itemize}
    \item Averaging: distributed estimation, opinion dynamics
    \item Distrbuted control in cooperative robotics
    \item Distributed optimization 
        \begin{itemize}
            \item distributed machine learning
            \item distributed decision-making in cooperative robotics
            \item distributed optimal control in energy systems and cooperative robotics
        \end{itemize}
\end{itemize}


\section{Measurement filtering in wireless sensor networks}
Consider a network of $N$ sensors with local sensing, computation and communication. Agent $i,i\in\{1,\dots,N\}$, takes a local measurement from the environment (temperature, pressure, etc.). Let $x_{i0}\in\R$ be the scalar local measurement
Agents are interested in agreeing on the average of the measurements, 
\[
    x_{\text{avg}}=\displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}x_{i0}
\]
to have a better estimate of the environment quantity

Consider the following "distributed algorithm" based on "local" linear averaging, for each $i\in\{1,\dots,N\}$
\begin{gather*}
    x_i^0=x_{i0}\\
    x_i^{k+1} = \text{average}(x_i^k,\{x_j^k,j\text{ "neighbour" of }i\}), \qquad k\in\N
\end{gather*}
generalizing coefficients of the update: 
\begin{gather*}
    x_i^0=x_{i0}\\
    x_i^{k+1} = \displaystyle\sum_{j=1}^{N} a_{ij}x_j^k \qquad k\in\N
\end{gather*}
\begin{remark}
    $a_{ij}\geq 0$ and $\sum_{j=1}^{N}a_{ij}=1$
\end{remark}
\begin{remark}
    $a_{ij}=0$, for some $j\in\{1,\dots,N\}$, i.e. $a_{ij}=0$ if $i$ does not have access to the estimate of $j$
\end{remark}


\section{Parameter Estimation in Wireless Sensor Networks}

Consider a network of $N$ sensors with local sensing, computation and communication aiming at estimating a common parameter $\theta^*\in\R$
Each sensor $i$ measures 
\[
    y_i=B_i\theta^* + v_i
\]
with $y_i\in\R^{m_1},B_i$ known matrix and $v_i$ a random measurement noise. Assume $v_1,\dots,v_N$ independent and Gaussian, with zero mean and covariance $E[v_iv_i^T]=\Sigma_i$. Assume $\sum_{i=1}^{N}m_i\geq m$ and $\begin{bmatrix}
    B_1 \\ \vdots \\ B_N
\end{bmatrix} $ full rank
Compute a least-squares estimate 
\[
    \hat\theta = \argmin_\theta \displaystyle\sum_{i=1}^{N}(y_i-B_i\theta)^T\Sigma_i^{-1}(y_i-B_i\theta)
\]
The optimal solution is 
\begin{align*}
    \hat\theta &= \left(\displaystyle\sum_{i=1}^{N}B_i^T\Sigma_i^{-1}B_i \right)^{-1} \displaystyle\sum_{i=1}^{N}B_i^T\Sigma_i^{-1}y_i\\ 
    & =  \left(\displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}B_i^T\Sigma_i^{-1}B_i \right)^{-1} \displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}B_i^T\Sigma_i^{-1}y_i\\ 
\end{align*}
The optimal solution can be obtained by computing two averages $\frac{1}{N}\displaystyle\sum_{i=1}^{N}\beta_i$ and $\frac{1}{N}\displaystyle\sum_{i=1}^{N}\beta_i$

\subsection{Opinion Dynamics in Social Influence Networks}
Group of $N$ individuals, with $x_i^k$ being the opinion of individual $i$ at time $k$. Opinions are updated according to 
\[
    x_i^{k+1} = \displaystyle\sum_{j=1}^{N}a_{ij}x_j^k
\]


\section{Main questions in averaging algorithms}
\begin{itemize}
    \item Do node estimates converge? Do they converge to a common value ("reach consensus")?
    \item Do they reach consensus to the average ("average consensus")?
    \item How can we model communication  in general networks?
    \item Can we answer the above questions for general networks and communication protocols?
    \item What assumptions do we need on the communication network?
\end{itemize}

\section{Distributed control in cooperative robotics}
Team of $N$ (mobile) robots aiming to execute complex tasks
\subsubsection{Basic tasks}
\begin{itemize}
    \item rendevous, containment
    \item formation, flocking
    \item coverage
\end{itemize}
\subsubsection{Complex tasks}
\begin{itemize}
    \item pickup and delivery
    \item surveillance and patrolling
    \item exploration
    \item satellite constellation
\end{itemize}
\subsection{Main questions in cooperative robotics}
\begin{itemize}
    \item Do robot states asymptotically converge? 
    \item Do the asympototic staes satisfy the global, desired task?
    \item How can we model communication in (general) robotic networks?
    \item What assumptions do we need on the communication network?
    \item Can we answer the above questions for general networks and communication protocols?
\end{itemize}
\subsection{Distributed optimal control}
\begin{align*}
    \min_{\substack{x_1,\dots,x_N \\ u_1,\dots,u_N}} & \displaystyle\sum_{i=1}^{N}(\displaystyle\sum_{\tau=0}^{T-1}\ell_i(z_{i,\tau},u_{i,\tau})+m_i(z_{i,T}))\\
    \text{subj to} & \displaystyle\sum_{i=1}^{N}H_iz_{i,\tau}\leq h, && \tau\in[0,T]\\ 
    & z_{i,\tau+1}=A_iz_{i,\tau}+B_iu_{i,\tau} && \forall i, \tau \in [0,T]\\ 
    & z_{i,\tau}\in Z_i, \quad u_{i,\tau}\in U_i, && \forall i, \tau \in [0,T]\\ 
\end{align*}









\chapter{Preliminaries on Algebraic Graph Theory}
\begin{definition}[Digraph]
    A digraph is a pair $G=(I,E)$ where $I={1,\dots,N}$ is a set of elements called \emph{nodes} and $E\subset I \times I$ is a set of ordered node pairs called \emph{edges}\\
    \emph{Edge}: the pair $(i,j)$ denotes an edge from $i$ to $j$\\ 
    \emph{Self-loop}: edge from a node to itself, i.e. $(i,i)$
\end{definition}
\begin{definition}[Undirected (di)graph]
    if for any $(i,j)\in E$ then $(j,i)\in E$  
\end{definition}
\begin{definition}[Subgraph]
    $(I',E')$ subgraph of $(I,E)$ if $I'\subset I$ and $E' \subset E$. Spanning subgraph if $I'=I$
\end{definition}
\begin{definition}[In-neighbours of $i$]$j\in I$ is an in-neighbour of $i\in I$ if $(j,i)\in E$
\end{definition}
\begin{definition}[Set of in-neighbours of $i$]
    $\mathcal{N}_i^{\text{IN}}=\{j\in\{1,\dots,N\}|(j,i)\in E\}$
\end{definition}
\begin{definition}[Out-neighbours of $i$]$j\in I$ is an out-neighbour of $i\in I$ if $(i,j)\in E$
\end{definition}
\begin{definition}[Set of out-neighbours of $i$]
    $\mathcal{N}_i^{\text{IN}}=\{j\in\{1,\dots,N\}|(i,j)\in E\}$
\end{definition}
\begin{definition}[In-degree $\deg_i^{\text{IN}}$]
    number of in-neighbours, i.e. carinality of $\mathcal{N}_i^{\text{IN}}(\deg_i^{\text{IN}}=|\mathcal{N}_i^{\text{IN}}|)$

    Out-degree analogous
\end{definition}
\begin{definition}[Balanced digraph]
    A digraph $G$ is balanced if $\deg_i^{\text{IN}}=\deg_i^{\text{OUT}}$ for all $i\in\{1,\dots,N\}$
\end{definition}
% TODO definitions slides 4-5-7-8-9-10

\begin{definition}[Complete graph]
    Unweighted graph such that $\forall i,j\ \exists \ (i,j),\ (j,i) \in E$
\end{definition}

\chapter{Averaging Systems}
\section{Distributed algorithm}
Given a network of $N$ agents communicating according to a fixed digraph $G$, i.e. each agent $i$ can receive messages only from in-neighbours in the graph, i.e. from $j\in\mathcal{N}_i^{\text{IN}}$. 
We start by considering a fixed graph, thus, each agent communicates with the same neighbours at each iteration $k\in\N$
\[
    x_i^{k+1}=\text{stf}_i(x_i^k,\{x_j^k\}_{j\in\mathcal{N}_i^{\text{IN}}}), \qquad i\in\{1,\dots,N\}
\]
where $\text{stf}_i$ is a function depending only on state $x_i$ and states $x_j,j\in\mathcal{N}_i^{\text{IN}}$.

Alternative version with out-neighbours:
\[
    x_i^{k+1}=\text{stf}_i(\{x_j\}_{j\in\mathcal{N}_i^{\text{OUT}}})
\]
\section{Discrete-time averaging systems}
Let $G^{\text{comm}}=(I,E)$ be a fixed (communication) digraph (self loops included). A linear averaging distributed algorithm can be written as:
\[
    x_i^{k+1}=\displaystyle\sum_{J\in\mathcal{N}_i^{\text{IN}}}a_{ij}x_j^k \qquad i\in\{1\dots,N\}
\]
where $x_i^k\in\R$ is the state of agent $i$ at $k$ and $a_{ij}>0$ are positive weights. 
\begin{remark}
    The weights $a_{ij}$ are defined only for $(i,j)\in E$
\end{remark}
Wach $i$ uses only the states of neighbours $j\in\mathcal{N}_i^{\text{IN}}$, thus distributed algorithm.

For analysis purposes, let us define weights $a_{ij}=0$ for $(j,i)\notin E$. Thus we can rewrite the distributed algorithm as 
\[
    x_i^{k+1}=\displaystyle\sum_{j=1}^{N}a_{ij}x_j^k \qquad i\in\{1,\dots,N\}
\]
This is a LTI autonomous system 
\[
    \begin{bmatrix}
        x_1^{k+1}\\ \vdots \\ x_N^{k+1}
    \end{bmatrix} = \begin{bmatrix}
        a_{11} & \cdots & a_{1N} \\ 
        \vdots & \ddots & \vdots \\
        a_{N1} & \cdots & a_{NN}
    \end{bmatrix} \begin{bmatrix}
        x_1^k \\ \vdots \\ x_N^k
    \end{bmatrix}
\]
Which can be compactly written as  
\[
    x^{k+1} 0 Ax^k
\]
\begin{remark}
    The matrix A can be seen as the weighted adjacency matrix of the reverse digraph $G^{\text{comm,rev}}$ of the digraph $G^{\text{comm}}$
\end{remark}
If instead of in-neighbours we use out-neighbours, we call the digraph a sensing digraph $G^{\text{sens}}$. In this case the notation becomes consistent with graph theory, so we get 
\[
    x^{k+1} = A x^k
\]
where $A$ can be seen as the weighted adjacency matrix of the sensing digraph $G^{\text{sens}}$

\section{Stochastic matrices}
The non-negative square matrix $A\in\R^{N\times N}$ is 
\begin{itemize}
    \item row stochastic if $A\mathbf{1}=\mathbf{1}$ (each row sums to 1)
    \item column stochastic if $A^\top \mathbf{1}=\mathbf{1}$ (each column sums to 1)
    \item doubly stochastic if both row and column stochastic.
\end{itemize}
\begin{lemma}
    Let $A$ be a row-stochastic matrix and $G$ the associate digraph. If $G$ is strongly connected and aperiodic, then 
    \begin{enumerate}
        \item the eigenvalue $\lambda=1$ is simple; 
        \item all the other eigenvalues $\mu$ satisfy $|\mu|<1$
    \end{enumerate}
\end{lemma}
\begin{remark}
    The condition "$G$ contains a globally reachable node and the subgraph of globally reachable noes is aperiodic" is necessary and sufficient
\end{remark}
\begin{theorem}[Consensus]
    Consider a (discrete-time) averaging system with associated digraph $G$ and wieghted adjacency matrix $A$. Assume $G$ is strongly connected and aperiodic, and $A$ is row stochastic. Then 
    \begin{enumerate}
        \item there exists a left eigenvector $w\in\R^N,w>0$ (i.e. with positive components $w_i>0$ for all $i=1,\dots,N$) such that 
            \[
                \lim_{k\to\infty}x^k = \mathbf{1}\displaystyle\frac{w^\top x^0}{w^\top \mathbf{1}} = \begin{bmatrix}
                    1 \\ \vdots \\ 1
                \end{bmatrix} \displaystyle\frac{\sum_{i=1}^{N}w_ix_i^0}{\sum_{i=1}^{N}w_i}
            \]
            i.e., consensus is reached to $ \displaystyle\frac{\sum_{i=1}^{N}w_ix_i^0}{\sum_{i=1}^{N}w_i}$
        \item if additionally $A$ is doubly stochastic, then 
            \[
                \lim_{k\to\infty}x^k = \begin{bmatrix}
                    1 \\ \vdots \\ 1
                \end{bmatrix} \displaystyle\frac{\sum_{i=1}^{N}x_i^0}{N}
            \]
            i.e., average consensus is reached
    \end{enumerate}
\end{theorem}

\section{Example: Metropolis-Hastings weights}
Given an undirected unweighted graph $G$ with edge set $E$ and degrees $d_1,\dots,d_n$
\[
    a_{ij} = \begin{cases}
        \displaystyle\frac{1}{1+\max\{d_i,d_j\}} & \text{if } (i,j)\in E \text{ and } i\neq j\\
        1- \displaystyle\sum_{h\in\mathcal{N}_i \backslash \{i\}}^{} a_{ih} & \text{if } i=j\\ 
        0 & \text{otherwise}
    \end{cases}
\]
Result: the matrix $A$ is symmetric and doubly-stochastic.

\section{Time-varying digraphs}
A time-varying digraph is a sequence of digraphs $\{G(k)\}_{k\geq 0}$.
\begin{remark}
    The main definitions of in/out neighbours, in/out degree, adjacency matrix can be generalized by considering time-varying versions, i.e. $\mathcal{N}_i^{\text{IN}}(k)$, $\mathcal{N}_i^{\text{OUT}}(k)$, $\deg_i^{\text{IN}}(k)$ ,$\deg_i^{\text{OUT}}(k)$, $A(k)$ associated to each graph $G(k)$. Connectivity requires new definitions as assuming each $G(k)$ to be connected is too conservative.
\end{remark}
\begin{definition}[Jointly strongly connected digraph]
    if $\bigcup_{\tau=k}^{+\infty}G(\tau)$ is strongly connecetd $\forall k \geq 0$
\end{definition}

\begin{definition}[Uniformly jointly strongly connected (or \emph{B}-strongly connected) digraph]
    if there exists $B\in\N$ such that $\bigcup_{\tau=k}^{k+B}G(\tau)$ is strongly connecetd $\forall k \geq 0$
\end{definition}

\begin{remark}
    The graph can be disconnected at some time $k$.
\end{remark}

\subsection{Averaging distributed algorithms over time-varying graphs}
Let $\{G(k)\}_{k\geq 0}$ be a time-varying digraph (with self loops for each $G(k)$). Consider the distributed algorithm 
\[
    x_i^{k+1} = \displaystyle\sum_{j\in\mathcal{N}_i^\text{IN}(k)}a_{ij}(k)x_j^k \qquad \forall i \in\{1,\dots,N\} 
\]
or the out-neighbours version 
\[
    x_i^{k+1} = \displaystyle\sum_{j\in\mathcal{N}_i^\text{OUT}(k)}a_{ij}(k)x_j^k \qquad \forall i \in\{1,\dots,N\} 
\]
where $x_i^k\in\R$ is the state of agent $i$ at $k$ and $a_{ij}(k)>0$. 

For analysis purposes, let us define weights $a_{ij}(k)=0$ for $(i,j)\notin E(k)$. Thus we can rewrite the distributed algorithm as 
\[
    x_i^{k+1} = \displaystyle\sum_{j=1}^{N} a_{ij}(k) x_j^k \qquad i\in\{1,\dots,N\}
\]
This is a Linear Time-Varying system 
\[
    x^{k+1} = A(k)x^k
\]
with state $x:=[x_1,\dots,x_N]^\top$ and state matrix
\[
    A(k) := \begin{bmatrix}
        a_11{k}& \cdots & a_{1N}(k) \\
        \vdots & \ddots & \vdots \\
        a_N1{k}& \cdots & a_{NN}(k) \\
    \end{bmatrix}
\]
being a weighted adjacency matrix associated to the digraph $G(k)$.
\subsection{Discrete-time consensus over time-varying graphs}
\begin{theorem}[]
    Let $\{A(k)\}_{k\geq 0}$ be a sequence of row-stochastic matrices with associated digraphs $\{G(k)\}_{k\geq 0}$. Assume 
    \begin{enumerate}
        \item each digraph $G(k)$ has a self-loop at each node; 
        \item each non-zero edge weight $a_{ij}(k)$, including the self-loop wights $a_{ii}(k)$, is larger than a constant $\epsilon>0$; 
        \item there exists $B\in\N$ such that, for all times $k\geq 0$, the union digraph $G(k)\cup \dots \cup G(k+B)$ is strongly connected. 
    \end{enumerate}
    Then 
    \begin{enumerate}
        \item there exists a non-negative vector $w\in\R^N$ such that the solution to $x^{k+1}=A(k)x^k$ converges (exponentially) to $\mathbf{1}\displaystyle\frac{w^\top x^0}{w^\top \mathbf{1}}$, i.e.
            \[
                \lim_{k\to\infty}x^k = \mathbf{1}\left(\displaystyle\frac{w^\top x^0}{w^\top \mathbf{1}}\right)
            \]
        \item if additionally each matrix in the sequence is doubly-stochastic, then 
            \[
                \lim_{k\to\infty} x^k = \mathbf{1} \displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}x_i^0
            \]
            i.e., average consensus is achieved
    \end{enumerate}
\end{theorem}

\section{Laplacian dynamics}
Consider a network of dynamical systems with dynamics 
\[
    \dot{x}(t) = u_i(t) \qquad i \in \{1,\dots,N\}
\]
with states $x_i\in\R$ and inputs $u_i\in\R$, communicating (or interacting) according to a digraph $G=(\{1,\dots,N\},E)$. Consider a (distributed) "proportional" feedback control 
\[
    u_i(t) = - \displaystyle\sum_{j\in\mathcal{N}_i^{\text{IN}}}a_{ij}(x_i(t)-x_j(t))
\]
or the out-neighbour version
\[
    u_i(t) = - \displaystyle\sum_{j\in\mathcal{N}_i^{\text{OUT}}}a_{ij}(x_i(t)-x_j(t))
\]
For analysis purposes, let us define weights $a_{ij}(k)=0$ for $(i,j)\notin E(k)$. Thus we can rewrite the distributed control systems as 
\[
    \dot{x}_i(t) = - \displaystyle\sum_{j=1}^{N} a_{ij} (x_i(t)-x_j(t)) \qquad \forall i\in\{1,\dots,N\}
\]
Defining $x:=[x_1 \cdots x_N]^\top$, it can be shown that it can be rewritten as the following Linear Time Invariant continuous-time system 
\[
    \dot{x}(t) = -Lx(t)
\]
where $L$ is the (weighted) Laplacian associated to the digraph $G$ with (weighted) adjacency matrix $A$ 

Let 
\[
    \dot{x}_i(t) = - \displaystyle\sum_{j=1}^{N} a_{ij} (x_i(t)-x_j(t)) \qquad \forall i\in\{1,\dots,N\}
\]
rearranging terms 
\[
    \dot{x}_i(t) = -\left(\displaystyle\sum_{j=1}^{N} a_{ij}\right) x_i(t) + \displaystyle\sum_{j=1}^{N} a_{ij}x_j(t) = -\deg_i^{\text{OUT}}x_i(t)+ (Ax(t))_i
\]
where $(Ax(t))_i$ is the $i$-th element of $Ax(t)$. Writing the previous dynamics in a compact form 
\[
    \dot{x}(t) = -(D^{\text{OUT}}-A)x(t)
\]
where we recall that $D^{\text{OUT}}$ is the (weighted) out-degree matrix. Recalling that $L=D^{\text{OUT}}-A$, it holds that 
\[
    \dot{x}(t) = -Lx(t)
\]
\begin{remark}
    if the in-neighbours version is considered, then $\dot{x}(t) = -L^{\text{IN}}x(t)$, where $L^{\text{IN}}=D^{\text{IN}}-A^T$ is the in-degree Laplacian (i.e. the Laplacian of the reverse graph of $G$)
\end{remark}
\subsection{Properties of the Laplacian matrix}
It can be easily verified that 
\[
    L\mathbf{1} = D^{\text{OUT}}\mathbf{1}-A\mathbf{1} = \begin{bmatrix}
        \deg_1^{\text{OUT}} \\ \vdots \\ \deg_i^{\text{OUT}}
    \end{bmatrix} - \begin{bmatrix}
        \deg_1^{\text{OUT}} \\ \vdots \\ \deg_i^{\text{OUT}}
    \end{bmatrix} = 0
\]
i.e., $\lambda=0$ is an eigenvalue of $L$ and $\mathbf{1}$ is an associated eigenvector.
\begin{lemma}
    Given a weighted digraph with Laplacian $L$, then all eigenvalues of $L$ different from zero have strictly positive real part
\end{lemma}

\begin{lemma}
    Given a weighted digraph with Laplacian $L$, the following statements are equivalent: 
    \begin{enumerate}
        \item $G$ is weight-balanced, i.e. $D^{\text{IN}} = D^{\text{OUT}}$
        \item $\mathbf{1}L = 0$
    \end{enumerate}
\end{lemma}
\begin{theorem}[]
    A weighted digraph with Laplacian $L$ contains a globally reachable node if and only if $\lambda=0$ is simple.
\end{theorem}
\begin{corollary}
    If a weighted digraph is strongly connected, then $\lambda=0$ is simple
\end{corollary}

\subsection{Consensus for Laplacian dynamics}
\begin{theorem}[]
    let $L$ be a (weighted) Laplacian matrix with associated strongly connected (weighted) digraph $G$. Consider the Laplacian dynamics $\dot{x}(t) = -Lx(t),\  t\geq0$, then
    \begin{enumerate}
        \item \[
                \lim_{t\to\infty} x(t) = \mathbf{1}\left(\displaystyle\frac{w^\top x(0)}{w^\top\mathbf{1}}\right)
        \]
            with $w^\top L = 0$, i.e. $w$ is a left eigenvector for the eigenvalue $\lambda=0$;
        \item if additionally $G$ is weight-balanced then 
            \[
                \lim_{t\to\infty}x(t) = \mathbf{1} \displaystyle\frac{\sum_{i=1}^{N}x_i(0)}{N}
            \]
    \end{enumerate}
\end{theorem}














\end{document}
