\documentclass{book}
\usepackage{amsmath, amsthm, graphicx, amsfonts, float, bm}
\usepackage[english]{babel}
\graphicspath{ {./images/} }

\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,237mm},
 left=20mm,
 top=30mm,
 }
 \usepackage[hidelinks]{hyperref}

\newcommand\at[2]{\left.#1\right|_{#2}}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\des}{des}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\notimplies}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\deriv}[1]{\displaystyle\frac{d}{d #1}}
\newcommand{\traj}{(\bar{\mathbf{x}},\bar{\mathbf{u}})}


\newtheoremstyle{theoremv2}{}{}{}{}{\bfseries}{}{\newline}{}
\theoremstyle{theoremv2}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheoremstyle{defv2}{}{}{}{}{\bfseries}{}{\newline}{}
\theoremstyle{defv2}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{remark}
\newtheorem*{notation}{Notation}
\theoremstyle{definition}
\newtheorem*{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem*{corollary}{Corollary}

\title{Distributed Autonomous Systems M}
\author{Dante Piotto}
\date{spring semester 2024}


\begin{document}
\maketitle
\tableofcontents

\chapter{Introduction and scenarios}
\section{Distributed Autonomous System}
Each agent $i\in\{1,\dots,N\}$ has 
\begin{itemize}
    \item local physical and/or cyber state $x_i$
    \item computational and sensing capabilities
    \item communication capability: exchange messages with "neighbours"
\end{itemize}


\section{Scenarios and applications of distributed systems}
\begin{itemize}
    \item Averaging: distributed estimation, opinion dynamics
    \item Distrbuted control in cooperative robotics
    \item Distributed optimization 
        \begin{itemize}
            \item distributed machine learning
            \item distributed decision-making in cooperative robotics
            \item distributed optimal control in energy systems and cooperative robotics
        \end{itemize}
\end{itemize}


\section{Measurement filtering in wireless sensor networks}
Consider a network of $N$ sensors with local sensing, computation and communication. Agent $i,i\in\{1,\dots,N\}$, takes a local measurement from the environment (temperature, pressure, etc.). Let $x_{i0}\in\R$ be the scalar local measurement
Agents are interested in agreeing on the average of the measurements, 
\[
    x_{\text{avg}}=\displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}x_{i0}
\]
to have a better estimate of the environment quantity

Consider the following "distributed algorithm" based on "local" linear averaging, for each $i\in\{1,\dots,N\}$
\begin{gather*}
    x_i^0=x_{i0}\\
    x_i^{k+1} = \text{average}(x_i^k,\{x_j^k,j\text{ "neighbour" of }i\}), \qquad k\in\N
\end{gather*}
generalizing coefficients of the update: 
\begin{gather*}
    x_i^0=x_{i0}\\
    x_i^{k+1} = \displaystyle\sum_{j=1}^{N} a_{ij}x_j^k \qquad k\in\N
\end{gather*}
\begin{remark}
    $a_{ij}\geq 0$ and $\sum_{j=1}^{N}a_{ij}=1$
\end{remark}
\begin{remark}
    $a_{ij}=0$, for some $j\in\{1,\dots,N\}$, i.e. $a_{ij}=0$ if $i$ does not have access to the estimate of $j$
\end{remark}


\section{Parameter Estimation in Wireless Sensor Networks}

Consider a network of $N$ sensors with local sensing, computation and communication aiming at estimating a common parameter $\theta^*\in\R$
Each sensor $i$ measures 
\[
    y_i=B_i\theta^* + v_i
\]
with $y_i\in\R^{m_1},B_i$ known matrix and $v_i$ a random measurement noise. Assume $v_1,\dots,v_N$ independent and Gaussian, with zero mean and covariance $E[v_iv_i^T]=\Sigma_i$. Assume $\sum_{i=1}^{N}m_i\geq m$ and $\begin{bmatrix}
    B_1 \\ \vdots \\ B_N
\end{bmatrix} $ full rank
Compute a least-squares estimate 
\[
    \hat\theta = \argmin_\theta \displaystyle\sum_{i=1}^{N}(y_i-B_i\theta)^T\Sigma_i^{-1}(y_i-B_i\theta)
\]
The optimal solution is 
\begin{align*}
    \hat\theta &= \left(\displaystyle\sum_{i=1}^{N}B_i^T\Sigma_i^{-1}B_i \right)^{-1} \displaystyle\sum_{i=1}^{N}B_i^T\Sigma_i^{-1}y_i\\ 
    & =  \left(\displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}B_i^T\Sigma_i^{-1}B_i \right)^{-1} \displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}B_i^T\Sigma_i^{-1}y_i\\ 
\end{align*}
The optimal solution can be obtained by computing two averages $\frac{1}{N}\displaystyle\sum_{i=1}^{N}\beta_i$ and $\frac{1}{N}\displaystyle\sum_{i=1}^{N}\beta_i$

\subsection{Opinion Dynamics in Social Influence Networks}
Group of $N$ individuals, with $x_i^k$ being the opinion of individual $i$ at time $k$. Opinions are updated according to 
\[
    x_i^{k+1} = \displaystyle\sum_{j=1}^{N}a_{ij}x_j^k
\]


\section{Main questions in averaging algorithms}
\begin{itemize}
    \item Do node estimates converge? Do they converge to a common value ("reach consensus")?
    \item Do they reach consensus to the average ("average consensus")?
    \item How can we model communication  in general networks?
    \item Can we answer the above questions for general networks and communication protocols?
    \item What assumptions do we need on the communication network?
\end{itemize}

\section{Distributed control in cooperative robotics}
Team of $N$ (mobile) robots aiming to execute complex tasks
\subsubsection{Basic tasks}
\begin{itemize}
    \item rendevous, containment
    \item formation, flocking
    \item coverage
\end{itemize}
\subsubsection{Complex tasks}
\begin{itemize}
    \item pickup and delivery
    \item surveillance and patrolling
    \item exploration
    \item satellite constellation
\end{itemize}
\subsection{Main questions in cooperative robotics}
\begin{itemize}
    \item Do robot states asymptotically converge? 
    \item Do the asympototic staes satisfy the global, desired task?
    \item How can we model communication in (general) robotic networks?
    \item What assumptions do we need on the communication network?
    \item Can we answer the above questions for general networks and communication protocols?
\end{itemize}
\subsection{Distributed optimal control}
\begin{align*}
    \min_{\substack{x_1,\dots,x_N \\ u_1,\dots,u_N}} & \displaystyle\sum_{i=1}^{N}(\displaystyle\sum_{\tau=0}^{T-1}\ell_i(z_{i,\tau},u_{i,\tau})+m_i(z_{i,T}))\\
    \text{subj to} & \displaystyle\sum_{i=1}^{N}H_iz_{i,\tau}\leq h, && \tau\in[0,T]\\ 
    & z_{i,\tau+1}=A_iz_{i,\tau}+B_iu_{i,\tau} && \forall i, \tau \in [0,T]\\ 
    & z_{i,\tau}\in Z_i, \quad u_{i,\tau}\in U_i, && \forall i, \tau \in [0,T]\\ 
\end{align*}









\chapter{Preliminaries on Algebraic Graph Theory}
\begin{definition}[Digraph]
    A digraph is a pair $G=(I,E)$ where $I={1,\dots,N}$ is a set of elements called \emph{nodes} and $E\subset I \times I$ is a set of ordered node pairs called \emph{edges}\\
    \emph{Edge}: the pair $(i,j)$ denotes an edge from $i$ to $j$\\ 
    \emph{Self-loop}: edge from a node to itself, i.e. $(i,i)$
\end{definition}
\begin{definition}[Undirected (di)graph]
    if for any $(i,j)\in E$ then $(j,i)\in E$  
\end{definition}
\begin{definition}[Subgraph]
    $(I',E')$ subgraph of $(I,E)$ if $I'\subset I$ and $E' \subset E$. Spanning subgraph if $I'=I$
\end{definition}
\begin{definition}[In-neighbours of $i$]$j\in I$ is an in-neighbour of $i\in I$ if $(j,i)\in E$
\end{definition}
\begin{definition}[Set of in-neighbours of $i$]
    $\mathcal{N}_i^{\text{IN}}=\{j\in\{1,\dots,N\}|(j,i)\in E\}$
\end{definition}
\begin{definition}[Out-neighbours of $i$]$j\in I$ is an out-neighbour of $i\in I$ if $(i,j)\in E$
\end{definition}
\begin{definition}[Set of out-neighbours of $i$]
    $\mathcal{N}_i^{\text{IN}}=\{j\in\{1,\dots,N\}|(i,j)\in E\}$
\end{definition}
\begin{definition}[In-degree $\deg_i^{\text{IN}}$]
    number of in-neighbours, i.e. carinality of $\mathcal{N}_i^{\text{IN}}(\deg_i^{\text{IN}}=|\mathcal{N}_i^{\text{IN}}|)$

    Out-degree analogous
\end{definition}
\begin{definition}[Balanced digraph]
    A digraph $G$ is balanced if $\deg_i^{\text{IN}}=\deg_i^{\text{OUT}}$ for all $i\in\{1,\dots,N\}$
\end{definition}
% TODO definitions slides 4-5-7-8-9-10

\begin{definition}[Complete graph]
    Unweighted graph such that $\forall i,j\ \exists \ (i,j),\ (j,i) \in E$
\end{definition}

\chapter{Averaging Systems}
\section{Distributed algorithm}
Given a network of $N$ agents communicating according to a fixed digraph $G$, i.e. each agent $i$ can receive messages only from in-neighbours in the graph, i.e. from $j\in\mathcal{N}_i^{\text{IN}}$. 
We start by considering a fixed graph, thus, each agent communicates with the same neighbours at each iteration $k\in\N$
\[
    x_i^{k+1}=\text{stf}_i(x_i^k,\{x_j^k\}_{j\in\mathcal{N}_i^{\text{IN}}}), \qquad i\in\{1,\dots,N\}
\]
where $\text{stf}_i$ is a function depending only on state $x_i$ and states $x_j,j\in\mathcal{N}_i^{\text{IN}}$.

Alternative version with out-neighbours:
\[
    x_i^{k+1}=\text{stf}_i(\{x_j\}_{j\in\mathcal{N}_i^{\text{OUT}}})
\]
\section{Discrete-time averaging systems}
Let $G^{\text{comm}}=(I,E)$ be a fixed (communication) digraph (self loops included). A linear averaging distributed algorithm can be written as:
\[
    x_i^{k+1}=\displaystyle\sum_{J\in\mathcal{N}_i^{\text{IN}}}a_{ij}x_j^k \qquad i\in\{1\dots,N\}
\]
where $x_i^k\in\R$ is the state of agent $i$ at $k$ and $a_{ij}>0$ are positive weights. 
\begin{remark}
    The weights $a_{ij}$ are defined only for $(i,j)\in E$
\end{remark}
Wach $i$ uses only the states of neighbours $j\in\mathcal{N}_i^{\text{IN}}$, thus distributed algorithm.

For analysis purposes, let us define weights $a_{ij}=0$ for $(j,i)\notin E$. Thus we can rewrite the distributed algorithm as 
\[
    x_i^{k+1}=\displaystyle\sum_{j=1}^{N}a_{ij}x_j^k \qquad i\in\{1,\dots,N\}
\]
This is a LTI autonomous system 
\[
    \begin{bmatrix}
        x_1^{k+1}\\ \vdots \\ x_N^{k+1}
    \end{bmatrix} = \begin{bmatrix}
        a_{11} & \cdots & a_{1N} \\ 
        \vdots & \ddots & \vdots \\
        a_{N1} & \cdots & a_{NN}
    \end{bmatrix} \begin{bmatrix}
        x_1^k \\ \vdots \\ x_N^k
    \end{bmatrix}
\]
Which can be compactly written as  
\[
    x^{k+1} 0 Ax^k
\]
\begin{remark}
    The matrix A can be seen as the weighted adjacency matrix of the reverse digraph $G^{\text{comm,rev}}$ of the digraph $G^{\text{comm}}$
\end{remark}
If instead of in-neighbours we use out-neighbours, we call the digraph a sensing digraph $G^{\text{sens}}$. In this case the notation becomes consistent with graph theory, so we get 
\[
    x^{k+1} = A x^k
\]
where $A$ can be seen as the weighted adjacency matrix of the sensing digraph $G^{\text{sens}}$

\section{Stochastic matrices}
The non-negative square matrix $A\in\R^{N\times N}$ is 
\begin{itemize}
    \item row stochastic if $A\mathbf{1}=\mathbf{1}$ (each row sums to 1)
    \item column stochastic if $A^\top \mathbf{1}=\mathbf{1}$ (each column sums to 1)
    \item doubly stochastic if both row and column stochastic.
\end{itemize}
\begin{lemma}
    Let $A$ be a row-stochastic matrix and $G$ the associate digraph. If $G$ is strongly connected and aperiodic, then 
    \begin{enumerate}
        \item the eigenvalue $\lambda=1$ is simple; 
        \item all the other eigenvalues $\mu$ satisfy $|\mu|<1$
    \end{enumerate}
\end{lemma}
\begin{remark}
    The condition "$G$ contains a globally reachable node and the subgraph of globally reachable noes is aperiodic" is necessary and sufficient
\end{remark}
\begin{theorem}[Consensus]
    Consider a (discrete-time) averaging system with associated digraph $G$ and wieghted adjacency matrix $A$. Assume $G$ is strongly connected and aperiodic, and $A$ is row stochastic. Then 
    \begin{enumerate}
        \item there exists a left eigenvector $w\in\R^N,w>0$ (i.e. with positive components $w_i>0$ for all $i=1,\dots,N$) such that 
            \[
                \lim_{k\to\infty}x^k = \mathbf{1}\displaystyle\frac{w^\top x^0}{w^\top \mathbf{1}} = \begin{bmatrix}
                    1 \\ \vdots \\ 1
                \end{bmatrix} \displaystyle\frac{\sum_{i=1}^{N}w_ix_i^0}{\sum_{i=1}^{N}w_i}
            \]
            i.e., consensus is reached to $ \displaystyle\frac{\sum_{i=1}^{N}w_ix_i^0}{\sum_{i=1}^{N}w_i}$
        \item if additionally $A$ is doubly stochastic, then 
            \[
                \lim_{k\to\infty}x^k = \begin{bmatrix}
                    1 \\ \vdots \\ 1
                \end{bmatrix} \displaystyle\frac{\sum_{i=1}^{N}x_i^0}{N}
            \]
            i.e., average consensus is reached
    \end{enumerate}
\end{theorem}

\section{Example: Metropolis-Hastings weights}
Given an undirected unweighted graph $G$ with edge set $E$ and degrees $d_1,\dots,d_n$
\[
    a_{ij} = \begin{cases}
        \displaystyle\frac{1}{1+\max\{d_i,d_j\}} & \text{if } (i,j)\in E \text{ and } i\neq j\\
        1- \displaystyle\sum_{h\in\mathcal{N}_i \backslash \{i\}}^{} a_{ih} & \text{if } i=j\\ 
        0 & \text{otherwise}
    \end{cases}
\]
Result: the matrix $A$ is symmetric and doubly-stochastic.

\section{Time-varying digraphs}
A time-varying digraph is a sequence of digraphs $\{G(k)\}_{k\geq 0}$.
\begin{remark}
    The main definitions of in/out neighbours, in/out degree, adjacency matrix can be generalized by considering time-varying versions, i.e. $\mathcal{N}_i^{\text{IN}}(k)$, $\mathcal{N}_i^{\text{OUT}}(k)$, $\deg_i^{\text{IN}}(k)$ ,$\deg_i^{\text{OUT}}(k)$, $A(k)$ associated to each graph $G(k)$. Connectivity requires new definitions as assuming each $G(k)$ to be connected is too conservative.
\end{remark}
\begin{definition}[Jointly strongly connected digraph]
    if $\bigcup_{\tau=k}^{+\infty}G(\tau)$ is strongly connecetd $\forall k \geq 0$
\end{definition}

\begin{definition}[Uniformly jointly strongly connected (or \emph{B}-strongly connected) digraph]
    if there exists $B\in\N$ such that $\bigcup_{\tau=k}^{k+B}G(\tau)$ is strongly connecetd $\forall k \geq 0$
\end{definition}

\begin{remark}
    The graph can be disconnected at some time $k$.
\end{remark}

\subsection{Averaging distributed algorithms over time-varying graphs}
Let $\{G(k)\}_{k\geq 0}$ be a time-varying digraph (with self loops for each $G(k)$). Consider the distributed algorithm 
\[
    x_i^{k+1} = \displaystyle\sum_{j\in\mathcal{N}_i^\text{IN}(k)}a_{ij}(k)x_j^k \qquad \forall i \in\{1,\dots,N\} 
\]
or the out-neighbours version 
\[
    x_i^{k+1} = \displaystyle\sum_{j\in\mathcal{N}_i^\text{OUT}(k)}a_{ij}(k)x_j^k \qquad \forall i \in\{1,\dots,N\} 
\]
where $x_i^k\in\R$ is the state of agent $i$ at $k$ and $a_{ij}(k)>0$. 

For analysis purposes, let us define weights $a_{ij}(k)=0$ for $(i,j)\notin E(k)$. Thus we can rewrite the distributed algorithm as 
\[
    x_i^{k+1} = \displaystyle\sum_{j=1}^{N} a_{ij}(k) x_j^k \qquad i\in\{1,\dots,N\}
\]
This is a Linear Time-Varying system 
\[
    x^{k+1} = A(k)x^k
\]
with state $x:=[x_1,\dots,x_N]^\top$ and state matrix
\[
    A(k) := \begin{bmatrix}
        a_11{k}& \cdots & a_{1N}(k) \\
        \vdots & \ddots & \vdots \\
        a_N1{k}& \cdots & a_{NN}(k) \\
    \end{bmatrix}
\]
being a weighted adjacency matrix associated to the digraph $G(k)$.
\subsection{Discrete-time consensus over time-varying graphs}
\begin{theorem}[]
    Let $\{A(k)\}_{k\geq 0}$ be a sequence of row-stochastic matrices with associated digraphs $\{G(k)\}_{k\geq 0}$. Assume 
    \begin{enumerate}
        \item each digraph $G(k)$ has a self-loop at each node; 
        \item each non-zero edge weight $a_{ij}(k)$, including the self-loop wights $a_{ii}(k)$, is larger than a constant $\epsilon>0$; 
        \item there exists $B\in\N$ such that, for all times $k\geq 0$, the union digraph $G(k)\cup \dots \cup G(k+B)$ is strongly connected. 
    \end{enumerate}
    Then 
    \begin{enumerate}
        \item there exists a non-negative vector $w\in\R^N$ such that the solution to $x^{k+1}=A(k)x^k$ converges (exponentially) to $\mathbf{1}\displaystyle\frac{w^\top x^0}{w^\top \mathbf{1}}$, i.e.
            \[
                \lim_{k\to\infty}x^k = \mathbf{1}\left(\displaystyle\frac{w^\top x^0}{w^\top \mathbf{1}}\right)
            \]
        \item if additionally each matrix in the sequence is doubly-stochastic, then 
            \[
                \lim_{k\to\infty} x^k = \mathbf{1} \displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}x_i^0
            \]
            i.e., average consensus is achieved
    \end{enumerate}
\end{theorem}

\section{Laplacian dynamics}
Consider a network of dynamical systems with dynamics 
\[
    \dot{x}(t) = u_i(t) \qquad i \in \{1,\dots,N\}
\]
with states $x_i\in\R$ and inputs $u_i\in\R$, communicating (or interacting) according to a digraph $G$. Consider a (distributed) "proportional" feedback control 
\[
    u_i(t) = - \displaystyle\sum_{j\in\mathcal{N}_i^{\text{IN}}}a_{ij}(x_i(t)-x_j(t))
\]
or the out-neighbour version
\[
    u_i(t) = - \displaystyle\sum_{j\in\mathcal{N}_i^{\text{OUT}}}a_{ij}(x_i(t)-x_j(t))
\]
For analysis purposes, let us define weights $a_{ij}(k)=0$ for $(i,j)\notin E(k)$. Thus we can rewrite the distributed control systems as 
\[
    \dot{x}_i(t) = - \displaystyle\sum_{j=1}^{N} a_{ij} (x_i(t)-x_j(t)) \qquad \forall i\in\{1,\dots,N\}
\]
Defining $x:=[x_1 \cdots x_N]^\top$, it can be shown that it can be rewritten as the following Linear Time Invariant continuous-time system 
\[
    \dot{x}(t) = -Lx(t)
\]
where $L$ is the (weighted) Laplacian associated to the digraph $G$ with (weighted) adjacency matrix $A$ 

Let 
\[
    \dot{x}_i(t) = - \displaystyle\sum_{j=1}^{N} a_{ij} (x_i(t)-x_j(t)) \qquad \forall i\in\{1,\dots,N\}
\]
rearranging terms 
\[
    \dot{x}_i(t) = -\left(\displaystyle\sum_{j=1}^{N} a_{ij}\right) x_i(t) + \displaystyle\sum_{j=1}^{N} a_{ij}x_j(t) = -\deg_i^{\text{OUT}}x_i(t)+ (Ax(t))_i
\]
where $(Ax(t))_i$ is the $i$-th element of $Ax(t)$. Writing the previous dynamics in a compact form 
\[
    \dot{x}(t) = -(D^{\text{OUT}}-A)x(t)
\]
where we recall that $D^{\text{OUT}}$ is the (weighted) out-degree matrix. Recalling that $L=D^{\text{OUT}}-A$, it holds that 
\[
    \dot{x}(t) = -Lx(t)
\]
\begin{remark}
    if the in-neighbours version is considered, then $\dot{x}(t) = -L^{\text{IN}}x(t)$, where $L^{\text{IN}}=D^{\text{IN}}-A^T$ is the in-degree Laplacian (i.e. the Laplacian of the reverse graph of $G$)
\end{remark}
\subsection{Properties of the Laplacian matrix}
It can be easily verified that 
\[
    L\mathbf{1} = D^{\text{OUT}}\mathbf{1}-A\mathbf{1} = \begin{bmatrix}
        \deg_1^{\text{OUT}} \\ \vdots \\ \deg_i^{\text{OUT}}
    \end{bmatrix} - \begin{bmatrix}
        \deg_1^{\text{OUT}} \\ \vdots \\ \deg_i^{\text{OUT}}
    \end{bmatrix} = 0
\]
i.e., $\lambda=0$ is an eigenvalue of $L$ and $\mathbf{1}$ is an associated eigenvector.
\begin{lemma}
    Given a weighted digraph with Laplacian $L$, then all eigenvalues of $L$ different from zero have strictly positive real part
\end{lemma}

\begin{lemma}
    Given a weighted digraph with Laplacian $L$, the following statements are equivalent: 
    \begin{enumerate}
        \item $G$ is weight-balanced, i.e. $D^{\text{IN}} = D^{\text{OUT}}$
        \item $\mathbf{1}L = 0$
    \end{enumerate}
\end{lemma}
\begin{theorem}[]
    A weighted digraph with Laplacian $L$ contains a globally reachable node if and only if $\lambda=0$ is simple.
\end{theorem}
\begin{corollary}
    If a weighted digraph is strongly connected, then $\lambda=0$ is simple
\end{corollary}

\subsection{Consensus for Laplacian dynamics}
\begin{theorem}[]
    let $L$ be a (weighted) Laplacian matrix with associated strongly connected (weighted) digraph $G$. Consider the Laplacian dynamics $\dot{x}(t) = -Lx(t),\  t\geq0$, then
    \begin{enumerate}
        \item \[
                \lim_{t\to\infty} x(t) = \mathbf{1}\left(\displaystyle\frac{w^\top x(0)}{w^\top\mathbf{1}}\right)
        \]
            with $w^\top L = 0$, i.e. $w$ is a left eigenvector for the eigenvalue $\lambda=0$;
        \item if additionally $G$ is weight-balanced then 
            \[
                \lim_{t\to\infty}x(t) = \mathbf{1} \displaystyle\frac{\sum_{i=1}^{N}x_i(0)}{N}
            \]
    \end{enumerate}
\end{theorem}

\chapter{Optimization basics}
\subsubsection{Convexity and gradient monotonicity}
If a convex function $\ell$ is also differentiable, then its gradient $\nabla\ell: \R^d\to\R^d$ satisfies 
\[
    \left(\nabla\ell(z_A)-\nabla\ell(z_B)\right)^\top(z_A-a_B)\geq 0
\]
for all $z_A,z_B$. That is, the gradient $\nabla\ell$ is a monotone operator

\subsubsection{Strict convexity and gradient monotonicity}
A function $\ell$ is strictly convex if for $z_A\neq z_B$ and $\theta\in (0,1)$ 
\[
    \ell(\theta z_A + (1-\theta)z_B)<\theta\ell(z_A)+(1-\theta)\ell(z_B)
\]
If the strictly convex function $\ell$ is also differentiable, then its gradient satisfies 
\[
    \left(\nabla\ell(z_A)-\nabla\ell(z_B)\right)^\top (z_A-z_B)>0
\]
for all $z_A,z_B$. That is, the gradient $\nabla\ell$ is a strictly monotone operator
\subsubsection{Strong convexity and gradient monotinicity}
A function $\ell$ is strongly convex with parameter $\mu>0$ if for $z_A\neq z_B$ and $\theta\in(0,1)$
\[
    \ell(\theta z_A + (1-\theta)z_B)<\theta\ell(z_A)+(1-\theta)\ell(z_B) - \mu\theta(1-\theta)\|z_A-z_B\|^2
\]
The gradient of a differentiable strongly convex function satisfies 
\[
    \left(\nabla\ell(z_A)-\nabla\ell(z_B)\right)^\top (z_A-z_B)\geq \mu\|z_A-z_B\|^2
\]
for all $z_A,z_B$. That is, the gradient $\nabla\ell$ is a strongly monotone operator

\subsubsection{Convexity and Lipschitz continuity of the gradient}
Consider a differentiable convex function $\ell$ with a Lipschitz continuous gradient with parameter $L>0$, i.e.
\[
    \|\nabla \ell(z_A)- \nabla\ell (z_B)\| \leq L\|z_A-z_B\|
\]
for all $z_A,z_B$.
Then, the following characterization holds
\[
    \left(\nabla\ell(z_A)-\nabla\ell(z_B)\right)^\top (z_A-z_B)\geq \displaystyle\frac{1}{L}\|\nabla\ell(z_A)-\nabla\ell(z_B)\|^2
\]
for all $z_A,z_B$. That is, the gradient $\nabla\ell$ is a co-coercive operator

\subsubsection{Strong convexity and Lipschitz continuity of the gradient}
Consider a strongly convex (with parameter $\mu>0$) function $\ell$ with Lipschitz continuous gradient (with parameter $L>0$). The the followin characterization holds 
\[
    \left(\nabla\ell(z_A)-\nabla\ell(z_B)\right)^\top (z_A-z_B)\geq \displaystyle\frac{\mu L }{\mu + L} \|z_A - z_B\|^2 + \displaystyle\frac{1}{\mu + L}\|\nabla\ell(z_A)-\nabla\ell(z_B)\|^2
\]
for all $z_A,z_B$.

\section{Optimization algorithms}
We consider optimization algorithms based on iterative descent.
\begin{notation}
    notatioWe denote by $z^k\in\R^d$ the estimate at iteration $k\in\N$ of a local minimum. 
\end{notation}
The algorithm starts at a given initial guess ecc ecc we know iterative descent
\subsection{A system theoretical perspective to the gradient method}
Let $\ell$ be $\mu$-strongly convex and with $L$-Lipschitz continuous gradient. The gradient method is a discrete-time integrator in feedback interconnection with a static map

\begin{align*}
    z^{k+1} &= z^k-\alpha u^k, \qquad  z^0 \text{ given}\\ 
    u^k &= \nabla \ell(z^k)
\end{align*}
where $u^k$ is a "control input" obtained through a static (nonlinear) state feedback. The block diagram is 
% TODO insert image slide 20

This is known as the \emph{Lur'e problem}

\section{steady-state analysis of the gradient method}
Assume that the state $z^k$ converges to some value $z_{eq}$. Then, such an equilibrium must satisfy: 
\begin{align*}
    z_{eq} = z_{eq}-\alpha\nabla\ell(z_{eq}) & \implies && z_{eq}:\nabla\ell(z_eq)\\ 
     & \implies && z_{eq} = z^*
\end{align*}
Consider the change of coordinates $z^k\to \tilde{z}^k-z_{eq}=z^k-z^*$. Then, the error dynamics is  
\begin{align*}
    \tilde{z}^{k+1} &= \tilde{z}^k-\alpha u^k \\
     u^k &= \nabla\ell(\tilde{z}^k+z^*)-\nabla\ell(z^*)
\end{align*}
where $u^k$ and $\tilde{z}^k$ satisfy, in light of the assumption on $\ell$, the following inequality\footnote{For all $z_A,z_B$ it holds that $$
    \left(\nabla\ell(z_A)-\nabla\ell(z_B)\right)^\top (z_A-z_B)\geq \displaystyle\frac{\mu L }{\mu + L} \|z_A - z_B\|^2 + \displaystyle\frac{1}{\mu + L}\|\nabla\ell(z_A)-\nabla\ell(z_B)\|^2
$$ } 
\[
    -(u^k)^T\tilde{z}^k \leq -\gamma_1\|\tilde{z}^k\|^2-\gamma_2\|u^k\|^2
\]
Consider a Lyapunov function $V:\R^d\to \R_{\geq0}$ given by $V(\tilde{z})=\|\tilde{z}\|^2$. Then 
\begin{align*}
    V(\tilde{z}^{k+1})-V(\tilde{z}^k) &= \|\tilde{z}^{k+1}\|^2-\|\tilde{z}^k\|^2\\
    &= \|\tilde{z}^k\|^2 - 2\alpha(u^k)^\top\tilde{z}^k+\alpha^2\|u^k\|^2-\|\tilde{z}^k\|^2\\ 
    &\leq -2\alpha\gamma_1 \|\tilde{z}^k\|^2 + \alpha(\alpha-2\gamma_2)\|u^k\|^2
\end{align*}
For a small enough stepsize $\alpha$ (i.e., $\alpha\leq 2\gamma_2$), we can write 
\begin{align*}
    V(\tilde{z}^{k+1})-V(\tilde{z}^k)<-2\alpha\gamma_1\|\tilde{z}^k\|^2 \quad \implies \quad \|\tilde{z}^{k+1}\|^2 & \leq (1-2\alpha\gamma_1) \|\tilde{z}^k\|^2\\ 
    & \leq (1-2\alpha\gamma_1)^k\|\tilde{z}^0\|^2
\end{align*}
Therefore $\{\tilde{z}^k\}_{k\in\\N}$ goes exponentially/geometrically fast to zero
\subsection{Gradient method for quadratic programs}
Consider a quadratic program 
\[
    \min_z \displaystyle\frac{1}{2} z^\top Q z + r^\top z
\]
With $Q=Q^\top>0$
The gradient method is an affine linear system 
\begin{align*}
    z^{k+1} &= z^k -\alpha(Qz^k+r) \qquad z^k \text{ given}\\ 
    &= (I-\alpha Q) z^k - \alpha r
\end{align*}
For a sufficiently small $\alpha$, the state matrix $(I-\alpha Q)$ is Schur. Hence, the state trajectory is \footnote{The geometric series $\sum_{i=0}^{\infty}\rho^i$ is equal to $(1-\rho)^{-1}$ for all $\rho<1$} 
\[
    z^k = (I-\alpha Q)^kz^0 - \alpha \displaystyle\sum_{i=0}^{k-1}(I-\alpha Q)^ir \to^{k\to\infty} -\alpha\left(\displaystyle\sum_{i=0}^{\infty}(I-^\alpha Q)^i\right)r = -Q^{-1}r
\]
\subsection{Gradient flow}
Let us swap the roles of the plant (the static nonlinearity) and the controller (the integrator)
% TODO insert images slide 24
We obtain the so-called gradient flow (continuous-time dynamics)
\[
    \dot{z}(t) = -\nabla \ell(z(t)) \qquad z(0) = z_0
\]
\begin{remark}
    A solution to the ODE exists if the vector field is Lipschitz continuous
\end{remark}
\subsection{Nesterov accelerated gradient method}
Consider the following two-step algorithm: for all $k\in\N$
\[
    \zeta^{k+1} = \zeta^k + \alpha_1(\zeta^k-\zeta^{k+1}) - \alpha_2\nabla\ell \left(\zeta^k + \alpha_1(\zeta^k-\zeta^{k+1})\right), \quad \zeta^0, \zeta^{-1} \text{ given}
\]
for some $\alpha_1,\alpha_2>0$. It admits the state-space representation
% TODO image slide 25
More general updates can also be considered: for all $k\in\N$
\[
    \zeta^{k+1} = \zeta^k + \alpha_1(\zeta^k-\zeta^{k+1})-\alpha_2\nabla\ell\left(\zeta^k+\alpha_3(\zeta^k-\zeta^{k-1})\right), \quad \zeta^0,\zeta^{-1} \text{ given}
\]
for some $\alpha_1, \alpha_2, \alpha_3>0$

\chapter{Parallel Optimization and Federated Learning}
% TODO slides 1-5
\subsubsection{Cost-coupled optimization for learning}
In learning applications, we usually consider optimization problems in the form 
\[
    \min_{z\in\R^d}\displaystyle\sum_{i=1}^{N}\ell_i(z)
\]
where, for all $i=1,\dots,N$, the cost function $\ell_i:\R^d\to \R$ is local and private 
\subsubsection{(Batch) gradient method for learning}
Consider the optimization problem 
\[
    \min_z \displaystyle\sum_{i=1}^{N}\ell_i(z)
\]
The (batch) gradient method is: for each iteration $k\in\N$ 
\[
    z^{k+1} = z^k -\alpha \displaystyle\sum_{i=1}^{N}\nabla\ell_i(z^k)
\]
\begin{remark}
    computation can be expensive
\end{remark}
\section{Incremental gradient method}
Consider the optimizatio problem
\[
    \min_z \displaystyle\sum_{i=1}^{N}\ell_i(z)
\]
Idea: rather than using the whole batch gradient at each $k\in\N$, just select one single "sample" per iteration. 

The \emph{incremental gradient method} is: for each iteration $k\in\N$
\[
    z^{k+1} = z^k - \alpha\nabla\ell_{i^k}(z^k)
\]
where $i^k\in\{1,\dots,N\}$
Two rules for choosing index $i^k$ at iteration $k$: 
\begin{itemize}
    \item Cyclic rule 
        \item Randomized rule
\end{itemize}
% TODO finish slide 4
\section{Stochastic Gradient Descent}
consider the stochastic optimization problem 
\[
    \min_z \mathbb{E}_{\mathcal{W}}[\ell(z,\mathcal{W})]
\]
where $\mathbb{E}_{\mathcal{W}}[\cdot]$ denotes the expected value with respect to the random variable $\mathcal{W}$ (possibly having an uknown probability distribution $p_{\mathcal{W}}(w))$
\begin{remark}
    for all $z$, also $\ell(z,\mathcal{W})$ is a random variable, whose probability distribution depends on $p_{\mathcal{W}}$ and $\ell$. Moreover, the gradient $\nabla\ell(z,\mathcal{W})$ at each $z$ is a random quantity
\end{remark}
Assumption: There exsists an oracle that, given a realization $\bar{w}$ of $\mathcal{W}$, returns the corresponding realization of the gradient $\nabla\ell(\bar{z},\bar{w})$ at any query point $\bar{z}$

The stochastic gradient descent is: for each iteration $k\in\N$ draw a realization $w^k$ of $\mathcal{W}$ and update 
\[
    z^{k+1} = z^k -\alpha\nabla\ell(z^k,w^k)
\]
\subsection{Convergence}
\begin{proposition}
    Assume: 
    \begin{itemize}
        \item $\ell$ is a $\mu$-strongly convex function with $L$-Lipschitz continuous gradient (uniformly in its second argument)
        \item $\nabla\ell(z,\mathcal{W})$ is an unbiased estimatee of $\nabla_z\mathbb{E}_{\mathcal{W}}[\ell(z,\mathcal{W})]$
        \item $\|\nabla\ell(z,\mathcal{W})\|\leq M$ almost surely\footnote{A sequence of random variables $\{\mathcal{X}_k\}_{k\in\N}$ converges almost surely to the rv $\mathcal{X}$ if $\mathbb{P}(\lim_{k\to\infty}\mathcal{X}_k=\mathcal{X})=1$} for some $M>0$
    \end{itemize}
\end{proposition} %TODO finish slide 6-7
\section{Beyond SGD: Adaptive Momentum Estimation (Adam)}
The ADAM algorithm reads as follows
\begin{itemize}
    \item Mean and Variance (first momentum and second momentum)
        \begin{align*}
            m^{k+1} & = \beta_1m^k+(1-\beta_1)\nabla\ell(z^k,w^k), &\text{for some } \beta\in(0,1)\\
            v^{k+1} & = \beta_2v^k+(1-\beta_2)[\nabla\ell(z^k,w^k)]^2, &\text{for some } \beta_2\in(0,1)
        \end{align*}
        where the square operation $[\cdot]^2$ is meant component-wise
    \item Construnct the descent direction
        \begin{align*}
            \hat{m} &= \displaystyle\frac{1}{1-\beta_1^{k+1}}m^{k+1}\\
            \hat{v} &= \displaystyle\frac{1}{1-\beta_2^{k+1}}v^{k+1}\\
            d^k &= -\displaystyle\frac{\hat{m}}{\sqrt{\hat{v}}+\epsilon}, \qquad \text{for some } \epsilon> 0
        \end{align*}
        where the division in the last equation is meant element-wise
    \item Update of the solution estimate 
        \[
            z^{k+1} = z^k + \alpha d^k
        \]
\end{itemize}

\section{Federated learning}
Consider the optimization problem 
\[
    \min_z \displaystyle\sum_{i=1}^{N}\ell(z;\mathcal{D}^i,p^i)
\]
Paradigm:
\begin{itemize}
    \item local private data $\mathcal{D}^i = ([\mathcal{D^i}]_1,[\mathcal{D^i}]_2,\dots,[\mathcal{D^i}]_d)$ and $p^i$
    \item learn common parameters $z^*\in\R^d$ (common neural network)
    \item communication with a parameter server
\end{itemize}

\section{Distributed learning}
Consider the optimization problem 
\[
    \min_z \displaystyle\sum_{i=1}^{N}\ell(z;\mathcal{D}^i,p^i)
\]
Paradigm:
\begin{itemize}
    \item local private data $\mathcal{D}^i = ([\mathcal{D^i}]_1,[\mathcal{D^i}]_2,\dots,[\mathcal{D^i}]_d)$ and $p^i$
    \item learn common parameters $z^*\in\R^d$ (common neural network)
    \item communication with neighbours only
\end{itemize}



\chapter{Leader Follower networks: Formation control}
\section{analogy with mass-spring systems}
Consider a platoon of $N$ masses such that each mass $i$ is connected with mass $i-1$ and $i+1$ through a spring with elastic constants respectively $a_{i-1,i} = a_{i,i-1}>0$ and $a_{i+1,i} = a_{i,i+1}>0$. Let $x_i\in \R$ be the position of mass $i$

The elastic force at mass $i$, $F_{e,i}(x)$ is given by 
\[
    F_{e,i}(x) = -a_{i,i-1}(x_i-x_{i-1})-a_{i,i+1}(x_i-x_{i+1})
\]
For each spring, we can write the associated elastic force as the negative gradient of the elastic (potential) energy, so that 
\[
    F_{e,i}(x) = -\displaystyle\frac{\partial}{\partial x_i}(\displaystyle\frac{1}{2}a_{i,i-1}\|x_i-x_{i-1}\|^2 + \displaystyle\frac{1}{2}a_{i,i+1}\|x_i-x_{i+1}\|^2)
\]
Let us suppose that each spring can be written as the parallel of two springs with elastic constants respectively $\frac{1}{2}a_{i,i-1}>0$ and $\frac{1}{2}a_{i,i+1}>0$.

Let $x_i\in\R$ be the positino of mass $i$. The elastic force at mass $i$, $F_{e,i}(x)$ is given by
\[
    F_{e,i} = -(\displaystyle\frac{1}{2}a_{i,i-1}+\displaystyle\frac{1}{2}a_{i,i-1})(x_i-x_{i-1}) -(\displaystyle\frac{1}{2}a_{i,i+1}+\displaystyle\frac{1}{2}a_{i,i+1})(x_i-x_{i+1}) 
\]
As before, we can write the elastic force as the negative gradient of the elastic (potential) energy, i.e.
\[
    F_{e,i}(x) = -\displaystyle\frac{\partial}{\partial x_i}(\displaystyle\frac{1}{2}\displaystyle\frac{a_{i,i-1}}{2}\|x_i-x_{i-1}\|^2 + \displaystyle\frac{1}{2}\displaystyle\frac{a_{i,i-1}}{2}\|x_i-x_{i-1}\|^2 + \displaystyle\frac{1}{2}\displaystyle\frac{a_{i,i+1}}{2}\|x_i-x_{i+1}\|^2 + \displaystyle\frac{1}{2}\displaystyle\frac{a_{i,i+1}}{2}\|x_i-x_{i+1}\|^2)
\]
The total elastic (potential) energy of the mass-spring system can be written as
\begin{align*}
    V(x) &= \displaystyle\sum_{i=1}^{N}\displaystyle\sum_{j\in \mathcal{N}_i} \displaystyle\frac{1}{2}\displaystyle\frac{a_{i,j}}{2}\|x_i-x_j\|^2\\ 
         &= \displaystyle\sum_{i=1}^{N}\displaystyle\sum_{j\in \mathcal{N}_i} V_{ij}(x_i,x_j)
\end{align*}
where we have defined $\mathcal{N}_i := \{ i-1, i+1 \}$ and $V_{ij}(x_i,x_j):=\displaystyle\frac{1}{2}\displaystyle\frac{a_{i,j}}{2}\|x_i-x_j\|^2$.

Thus, the elastic force at mass $i$ can be seen as the negative gradient of the energy, i.e. 
\begin{align*}
    F_{e,i}(x) &= -\displaystyle\frac{\partial}{\partial x_i}\displaystyle\sum_{j\in\mathcal{N}_i}\left(V_{ij}(x_i,x_j)+V_{ji}(x_j,x_i)\right)\\
               &=-\displaystyle\frac{\partial}{\partial x_i} V(x) 
\end{align*}
This formulation can be extended to more general systems in which masses are interconnected according to a topology described by an undirected graph $G=(\{1,\dots,N\},E)$

By adding a damping term on each mass, the system dynamics can be written as 
\begin{align*}
    \dot{x}_i &= v_i\\
    m_i\dot{v}_i &= -v_i - \displaystyle\frac{\partial}{\partial x_i}V(x) \quad \forall i \in \{ 1,\dots,N \}
\end{align*}
where we have considered the damping coeffiecient equal to one. 

If we assume that masses are small, we may written
\[
    v_i \approx - \displaystyle\frac{\partial}{\partial x_i} V(x)
\]
so that the dynamics may be approximated by the following firt order dynamics
\[
    \dot{x}_i = - \displaystyle\frac{\partial}{\partial x_i}V(x) \quad \forall i\in\{ 1,\dots,N \}
\]

Consider a network of $N$ agents communicating/interacting according to a fixed, undirected graph $G$. Let $x_i(t)\in\R^d$ be the state of agent $i$. Let agents run a Laplacian dynamics
\[
    \dot{x}_i = - \displaystyle\sum_{j\in\mathcal{N}_i}a_{ij}(x_i-x_j) \quad \forall i\in\{ 1,\dots,N \}
\]
We can rewrite it as 
\[
    \dot{x}_i(t) = - \displaystyle\sum_{j\in \mathcal{N}_i} \displaystyle\frac{\partial}{\partial x_i}\left(V_{ij}(x_i,x_j)+V_{ji}(x_j,x_i)\right) \quad \forall i \in \{ 1,\dots,N \}
\]
with $V_{ij}(x) = \displaystyle\frac{1}{2}\displaystyle\frac{a_{i,j}}{2}\|x_i-x_j\|^2$.

By recalling the definition of the total energy
\[
    V(x)=\displaystyle\sum_{i=1}^{N}\displaystyle\sum_{j\in\mathcal{N}_i}V_{ij}(x_i,x_j)
\]
the Laplacian dynamics 
\[
    \dot{x} = -Lx
\]
can be seen as a "gradient flow", i.e. 
\[
    \dot{x} = -\nabla V(x)
\]
Thus, the consensus configuration can be seen as a stationary point of $V$. This idea can be extended to general potential functions and applied to distributed control systems.

Consider a network of $N$ autonomous agents communicating/interacting according to a fixed, undirected graph. Let $x_i(t)\in\R^d$ be the state of agent $i$. Consider a global potential function defined as 
\[
    V(x)=\displaystyle\sum_{i=1}^{N}\displaystyle\sum_{j\in\mathcal{N}_i}V_{ij}(x_i,x_j)
\]
such that (local) minima of the potential correspond to desired configurations of the team. The gradient flow dynamics $\dot{x} = -\nabla V(x)$ turns out to be distributed. That is,
 \[
     \dot{x}_i(t) = \displaystyle\sum_{j\in\mathcal{N}_i}\displaystyle\frac{\partial}{\partial x_i}\left(V_{ij}(x_i,x_j)+V_{ji}(x_j,x_i)\right) \qquad \forall i \in \{ 1,\dots,N \}
 \]

 We can define a desired formation by assigning a set of distances, $d_{ij}$, between neighbouring agents $i$ and $J$ in a suitable graph 
 
 The main idea for formation control is to define a potential function matching the sparsity of $G$, $V^{\text{form}}(x) = \sum_{i=1}^{N}\sum_{j\in\mathcal{N}_i}V_{ij}^{\text{form}}(i,j)$, such that a configuration $x^{text{form}}$ satisfying 
 \[
     \|x_i^{\text{form}}-x_j^{\text{form}}\| = d_ij \qquad \forall (i,j)\in E
 \]
 is a minimum of $V$.

 In order to reach a formation with assigned distances $d_{ij}$, let us define 
 \[
     V_{ij}^{\text{form}}(x) = \displaystyle\frac{1}{8} \left(\|x_i-x_j\|^2-d_{ij}^2\right)^2
 \]
 with corresponding (global) potential function $V^{\text{form}}(x) = \displaystyle\sum_{i=1}^{N}\displaystyle\sum_{j\in\mathcal{N}_i}V_{ij}^\text{form}(x_i,x_j)$.

 The gradient flow dynamics of each agent $i$ is given by 
 \[
     \dot{x}_i = - \displaystyle\sum_{j\in\mathcal{N}_i}\displaystyle\frac{\partial}{\partial x_i} \left(V_{ij}^\text{form}(x_i,x_j)+V_{ji}(x_j,x_i)\right) \qquad \forall i \in \{1,\dots,N\}
 \]
 which reads as 
 \[
     \dot{x}_i = - \displaystyle\sum_{j\in\mathcal{N}_i} \left(\|x_i-x_j\|^2-d_{ij}^2\right)(x_i-x_j) \qquad \forall i \in \{1,\dots,N\}
 \]
This dynamics has multiple equilibrium points, including the desired formation in which the agents are at the assigned distances. In particular, the consensual solution $x_1=x_2=\dots=x_N$ is an (undesired) equilibrium.

Such a "degenerate" equilibrium can be avoided by means of additional "collision avoidance" potential functions $V_{ij}^\text{ca}(x_i,x_j)$ such that 
\[
    \lim_{\|x_i-x_j\|\to 0} V_{ij}^\text{ca}(x_i,x_j)=+\infty
\]
A possible solution is a barrier function given 
\[
    V_{ij}^\text{ca} = -\log(x_i-x_j)
\]
Similarly, barrier potential functions $V^\text{obs}(x_i)$, depending only on the state of agent $x_i$, can be used to avoid obstacles. 

The formation control dynamics becomes 
\[
    \dot{x}_i = - \displaystyle\frac{\partial V^\text{form}}{\partial x_i} - \displaystyle\frac{\partial V^\text{ca}(x)}{\delta x_i} - \nabla V^\text{obs}(x) \qquad \forall i\in\{1,\dots,N\}
\]


\chapter{Distributed Aggregative Optimization}
Consider $N$ robots in the plane that want to optimize their positions $z_i\in\R^2$, for all $i=1,\dots,N$ to perform multi-robot surveillance. Let:
\begin{itemize}
    \item $r_0\in\R^2$ be a target to protect 
    \item $r_i\in\R^2$ be the intruder associated to robot $i$ 
    \item $\sigma(z) = \displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N}z_i$ is the barycenter of the robots 
    \item Local cost function of robot $i$ 
        \[
            \ell_i(z_i,\sigma(z)) = \gamma_i\|z_i-r_i\|^2 + \|\sigma(z)-r_0\|^2
        \]
\end{itemize}
with $z\in\R^{2N}$ the stack of $z_1,\dots,z_N$ and $\gamma_i>0$ being a tradeoff parameter.
\section{Aggregative optimization}
Let us consider aggregative optimization problems in the form 
\[
    \min_{z_1,\dots,z_N} \displaystyle\sum_{i=1}^{N} \ell_i(z_i,\sigma(z))
\]
where the aggregative variable $\sigma(z)$ is defined as 
\[
    \sigma(z) = \displaystyle\frac{1}{N}\displaystyle\sum_{i=1}^{N} \phi_i(z_i)
\]
where 
\begin{itemize}
    \item $z=(z_1,\dots,z_N)$, with each $z_i\in\R^{n_i}$, for all $i=1,\dots,N$
    \item $\ell_i:\R^{n_i}\times\R^d\to\R$ and $\phi_i\R^{n_i}\to\R^d$, for all $i=1,\dots,N$
\end{itemize}
For scalar states, $z_i\in\R$, the \emph{centralized gradient} method at iteration $k$ reads as 
\[
    z_i^{k+1} = z_i^k -\alpha \displaystyle\frac{\partial}{\partial z_i}\left. \left(\displaystyle\sum_{j=1}^{N}\ell_j(z_j,\sigma(z_1,\dots,z_N))\right)\right|_{z_1 = z_1^k,\dots,z_N = z_N^k}
\]
for all $i=1,\dots,N$ where $\alpha>0$ is the stepsize
\subsubsection{Gradient computation (scalar case)}
Since the cost function is a composite function, we need the chain rule to compute its derivative with respect to $z_i$ 
\begin{align*}
    &\displaystyle\frac{\partial}{\partial z_i}\left.\left(\displaystyle\sum_{j=1}^{N}\ell_j(z_j,\sigma(z_1,\dots,z_N))\right)\right|_{z_1 = z_1^k,\dots,z_N = z_N^k} \\
    =& \left.\displaystyle\frac{\partial}{\partial z_i} \ell_i(z_i,\sigma)\right|_{z_i=z_i^k, \sigma=\frac{1}{N}\sum_{j=1}^{N}\phi_j(z_j^k)}  \\
    &+\left. \sum_{j=1}^{N}\displaystyle\frac{\partial}{\partial\sigma}l_j(z_j,\sigma)\right|_{z_j=z_j^k, \sigma=\frac{1}{N}\sum_{j=1}^{N}\phi_j(z_j^k)}\cdot \left.\displaystyle\frac{\partial\sigma(z_1,\dots,z_N)}{\partial z_i}\right|_{z_1=z_1^k,\dots,z_N = z_N^k}
\end{align*}
Notice that $\displaystyle\frac{\partial\sigma(z_1,\dots,z_N)}{\partial z_i}=\displaystyle\frac{1}{N}\displaystyle\frac{d}{d z_i}\phi_i(z_i)$ can be computed locally

As in the scalar case, we use the chain rule to compute the gradient of the composite function. The $i$-th block of the gradient, denoted as $\left[\nabla\left(\displaystyle\sum_{j=1}^{N}\ell_j(z_j,\sigma(z_1,\dots,z_N))\right)\right]_i\in\R^{n_i}$, is given by
\begin{align*}
    &\left[\nabla\left(\displaystyle\sum_{j=1}^{N}\ell_j(z_j,\sigma(z_1,\dots,z_N))\right)\right]_i \\ 
    =& \left.\nabla_1\ell_i(z_i,\sigma)\right|_{z_j=z_j^k, \sigma=\frac{1}{N}\sum_{j=1}^{N}\phi_j(z_j^k)} \\ 
    &+ \left.\displaystyle\frac{1}{N}\nabla \phi_i(z_i)\right|_{z_i=z_i^k}\cdot \left.\displaystyle\sum_{j=1}^{N}\nabla_2 \ell_j(z_j,\sigma)\right|_{z_j=z_j^k, \sigma=\frac{1}{N}\sum_{j=1}^{N}\phi_j(z_j^k)}
\end{align*}
\section{Distributed aggregative optimization}
In a distributed contex, each agent $i$ 
\begin{itemize}
    \item knows only $\ell_i$ and $\phi_i$
        \item maintains an estimate $z_i^k$ of $z_i^\star$
        \item maintains an estimate $s_i^k$ of $\phi(z^k) = \displaystyle\frac{1}{N}\displaystyle\sum_{j=1}^{N}\phi_j(z_j^k)$
        \item maintains an estimate $v_i^k$ of $\displaystyle\sum_{j=1}^{N}\nabla_2 \ell_j(z_j^k,\sigma(z^k))$
\end{itemize}
The "tracking" idea of gradient tracking algorithm is applied to aggregative optimization
\begin{align*}
    & z_i^{k+1} = z_i^k - \alpha\left( \nabla_1\ell_i(z_i^k,s_i^k) + \nabla \phi_i(z_i^k)v_i^k \right) && z_i^0 \in\R^{n_i}\\
    & s_i^{k+1} = \displaystyle\sum_{j\in\mathcal{N}_i}a_{ij}s_j^k + \phi_i(z_i^{k+1})-\phi_i(z_i^k) && s_i^0 = \phi_i(z_i^0)\\
    & v_i^{k+1} = \displaystyle\sum_{j\in\mathcal{N}_i}a_{ij}v_j^k + \nabla_2\ell_i(z_i^{k+1},s_i^{k+1})-\nabla_2\ell_i (z_i^k,s_i^k) && v_i^0 = \nabla_2\ell_i(z_i^0,s_i^0)
\end{align*}
\begin{theorem}[aggregative tracking distributed optimization algorithm: convergence]
    Assume $G$ is a strongly connected and aperiodic digraph, and $A$ is doubly stochastic. Assume that each function $\ell_i$ is strongly convex, the gradients $\nabla_1\ell_i$ and $\nabla_2\ell_i$ are Lipschitz continuous, and $\phi_i$ is differentiable and Lipschitz continuous. 

    Then, there exists $\alpha^\star$ such that for all $\alpha\in(0,\alpha^\star)$ the sequences of local solution estimates $\{ z_1^k,\dots,z_N^k \}_{k\in\N}$ generated by the aggregative tracking distributed opitmization algorithm satisfy 
    \[
        \lim_{k\to\infty}\|z_i^k-z_i^\star\| = 0
    \]
    at a linear rate, for all $i=1,\dots,N$
\end{theorem}
\subsection{Extension to online aggregative optimization}
Consider a time-varying instance of the problem 
\begin{align*}
    \min_{z} & \displaystyle\sum_{i=1}^{N}\ell_i^k(z_i,\sigma^k(z))\\
    \text{subj. to }& z_i\in Z_i^k, \qquad \forall i=1,\dots,N
\end{align*}
where $Z_i^k\subset \R^{n_i}$ are local constraint sets.

The goal is to design an algorithm generating a sequence $\{z_i^k\}_{k\in\N}$ that "tracks" the solution $z^{k,\star}=(z_1^{k,\star},\dots,z_N^{k,\star})$ of the $k-th$ problem instance.

% Algorithm on slide 10 not required for exam
\begin{remark}
    A regret $R_K$ can be introduced for the analysis. Under suitable assumptions, it can be proven that % TODO
\end{remark}


\chapter{Learning with Neural Networks}
In supervised learning we have labeled data whose generic element(sample) is composed by 
\begin{itemize}
    \item an input point $\mathcal{D}\in\R^d$, i.e. a vector $\mathcal{D} = ([\mathcal{D}]_1,\dots,[\mathcal{D}]_d)$ 
        \item a lebel $p\in\R$ associated to the input point
\end{itemize}
A dataset is usually made by $\mathcal{M}$ samples, i.e. $((\mathcal{D}^1,p^1),\dots,(\mathcal{D}^\mathcal{M},p^\mathcal{M}))$

Goal: reconstruct the uknown input-output map between inputs and labels\\
Strategy: approximate the map as a nonlinear function $\phi(\mathbb{u};\cdot):\R^d\to\R$ parametrized byh $\mathbb{u}$\\
Problem: find the best parameters $\mathbb{u}^\star$ based on data

\section{Basic element: the neuron model}
A generic neuron $h$ is a computational unit that 
\begin{itemize}
    \item has a set of weights $u_h\in\R^d$ 
    \item elaborates a vector $x\in\R^d$
    \item computes a scalar quantity 
        \[
            x_h^+ = \phi(x^\top u_h)
        \]
        with $\sigma:\R\to\R$ being the \emph{activation function}
\end{itemize}


\chapter{Multi-Robot Safety Controllers}
Consider a nonlinear system 
\[
    \dot{x}(t) = f(x(t),u(t)), \qquad x\in\R^n \quad u\in U\subset \R^m
\]
Let us define a safe (state) set 
\[
    X^s := \{ x\in\R^n | V^s(x)\geq0 \}
\]
for some sufficiently regular function $V^s:\R^n\to\R$

Goal: design a feedback control law $\kappa(x),\kappa:\R^n\to\R^m$ such that the set $X^s$ is forward invariant

\section{Control Barrier Functions}
Consider the time derivative of $V^s(x(t))$ along the system trajectories
\begin{align*}
    \displaystyle\frac{d}{dt}V^s(x(t)) &= \nabla V^s(x(t))^\top f(x(t)) + \displaystyle\sum_{h=1}^{m}\nabla V^s(x(t))^\top g_h(x(t))u_h(x(t))\\
                                       &= L_fV^s(x(t)) + L_gV^s(x(t))u(t)
\end{align*}
where $L_f$ and $L_g$ represent Lie derivatives.
We say that $V^s$ is a Control Barrier Function (CBF) if there exists a continuous, strictly increasing function $\gamma:\R\to\R$, with $\gamma(0)=0$, such that 
\[
    \sup_{u\in U}\{ L_fV^s(x) + L_gV^s(x)u + \gamma(V^s(x))\geq 0 \}
\]
for all $x\in\R^n$.

The above inequality induced by a CBF is called a \emph{control barrier certificate}.
\subsection{Admissible Control Space}
We can define the set of admissible (safe) controllers for a given state $x$ as 
\[
    U^s(x) = \left\{ u\in\R^m | L_fV^s(x) + L_gV^s(x)u + \gamma(V^s(x))\geq 0 \right\}
\]
i.e., the set of inputs satisfying the control barrier certificates. 
\begin{remark}
    It can be shown that (sufficiently regular) feedback controllers satisfying the above condition render $X^s$ forward invariant and asymptotically stable.
\end{remark}

\subsection{Safety Filters via Control Barrier Certificates}
Let $u^\text{ref}\in\R^m$ be a (possibly unsafe) reference input, e.g., obtained by a higher-level controller. The safety controller is designed to be "minimally invasive", i.e. it aims at altering the reference controller as little as possible. A safe policy $u = \kappa(x)$ can be designed as 
\begin{align*}
    \kappa(x) = \arg &\min_{u\in\R^m} \|u-u^\text{ref}(x)\|^2\\
                     &\text{subj. to } -L_fV^s(x) - L_gV^s(x)u - \gamma(V^s(x))\leq 0
\end{align*}
















\end{document}
