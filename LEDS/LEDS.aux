\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{5}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}What is a system?}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Learning modesl from data}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Types of learning}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}fields related to learning from data}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}System identification}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Grey box approach}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Learning steps}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Stochastic Processes}{9}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}stationary stochastic processes}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}cross-correlation and cross-covariance}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}vector stochastic processes}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}gaussian processes}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}white processes}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Stochastic models}{11}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Matlab stuff}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Estmation problem}{13}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Linear regression}{15}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}The Least Squares Method}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Geometrical interpretation of the LS estimate}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Statistical properties of the LS estimator}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}LS estimation fo ARX models}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Identifiablity}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Statistical properties}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}ARX optimal (one step ahead) predictor}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}LS estimation of AR models}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Recursive least squares}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{RLS I}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{RLS II}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Matrix inversion lemma (Woodbury identity)}{20}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{RLS III}{20}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{RLS IV}{20}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Initialization}{20}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Asymptotic behaviour of the RLS algorithm}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Recursive wighted least squares}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{RWLS I}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Prediction error methods}{23}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.1}Identification of ARMAX models}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.2}Statistical properties of PEM estimators}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.3}MISO ARX models}{26}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Statistical hypothesis testing}{27}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Model complexity selection and regularization}{29}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.0.1}The F-test}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.0.2}The final prediction error (FPE) criterion}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.0.3}Criteria with complexity terms}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.0.4}Akaike information criterien (AIC)}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Minimum description lenght (MDL) criterion}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {9}model assesment (validation)}{33}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Whiteness test}{33}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Test of cross-correlation}{34}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Maximum likelihood estimation}{35}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}the gaussian case}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}the Cram√©r-Rao lower bound}{36}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Classification: probabilistic models}{37}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}The Bayes classifier}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Logistic regression}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How to classify a new input $u(t)$?}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.1}The gradient descent algorithm}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Stochastic gradient descent}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Confusion matrix}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.2}Multiclass problems}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.3}Dealing with nonlinear boundaries}{41}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.4}Linear discriminant analysis}{41}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.5}Gaussian class densities with commmon covariance matrix}{42}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Multiclass problem}{42}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.6}Gaussian class denisities with different covariance matrices}{42}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Classification: deterministic models}{43}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.0.1}Separating hyperplanes}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.0.2}the maximum margin classifier}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Constrained optimization problems: the Lagrange multipliers}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Finding the maximum margin classifier by using the Lagrange multipliers}{46}{}\protected@file@percent }
\newlabel{Jderiv}{{12.1}{46}}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Support vector machine}{47}{}\protected@file@percent }
\newlabel{Jpartials}{{12.3}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.1}Dealing with nonlinear boundaries: the kernel trick}{49}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How does it work?}{49}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.2}Multiclass problems}{49}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{One-vs-all approach}{49}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{One-vs-one approach}{50}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Regularization}{51}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {13.1}Regularized least squares: the Ridge regression}{51}{}\protected@file@percent }
\gdef \@abspage@last{51}
