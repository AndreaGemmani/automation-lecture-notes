\babel@toc {english}{}\relax 
\contentsline {chapter}{\numberline {1}Introduction}{5}{}%
\contentsline {section}{\numberline {1.1}What is a system?}{5}{}%
\contentsline {subsection}{\numberline {1.1.1}Learning modesl from data}{6}{}%
\contentsline {section}{\numberline {1.2}Types of learning}{6}{}%
\contentsline {section}{\numberline {1.3}fields related to learning from data}{6}{}%
\contentsline {subsection}{\numberline {1.3.1}System identification}{6}{}%
\contentsline {subsubsection}{Grey box approach}{7}{}%
\contentsline {section}{\numberline {1.4}Learning steps}{7}{}%
\contentsline {chapter}{\numberline {2}Stochastic Processes}{9}{}%
\contentsline {section}{\numberline {2.1}stationary stochastic processes}{9}{}%
\contentsline {subsection}{\numberline {2.1.1}cross-correlation and cross-covariance}{9}{}%
\contentsline {section}{\numberline {2.2}vector stochastic processes}{9}{}%
\contentsline {section}{\numberline {2.3}gaussian processes}{9}{}%
\contentsline {section}{\numberline {2.4}white processes}{9}{}%
\contentsline {chapter}{\numberline {3}Stochastic models}{11}{}%
\contentsline {section}{\numberline {3.1}Matlab stuff}{11}{}%
\contentsline {chapter}{\numberline {4}Estmation problem}{13}{}%
\contentsline {chapter}{\numberline {5}Linear regression}{15}{}%
\contentsline {section}{\numberline {5.1}The Least Squares Method}{15}{}%
\contentsline {subsection}{\numberline {5.1.1}Geometrical interpretation of the LS estimate}{16}{}%
\contentsline {subsection}{\numberline {5.1.2}Statistical properties of the LS estimator}{16}{}%
\contentsline {subsection}{\numberline {5.1.3}LS estimation fo ARX models}{17}{}%
\contentsline {subsubsection}{Identifiablity}{17}{}%
\contentsline {subsubsection}{Statistical properties}{17}{}%
\contentsline {subsection}{\numberline {5.1.4}ARX optimal (one step ahead) predictor}{18}{}%
\contentsline {subsection}{\numberline {5.1.5}LS estimation of AR models}{18}{}%
\contentsline {section}{\numberline {5.2}Recursive least squares}{18}{}%
\contentsline {subsubsection}{RLS I}{19}{}%
\contentsline {subsubsection}{RLS II}{19}{}%
\contentsline {subsubsection}{Matrix inversion lemma (Woodbury identity)}{20}{}%
\contentsline {subsubsection}{RLS III}{20}{}%
\contentsline {subsubsection}{RLS IV}{20}{}%
\contentsline {subsubsection}{Initialization}{20}{}%
\contentsline {subsection}{\numberline {5.2.1}Asymptotic behaviour of the RLS algorithm}{21}{}%
\contentsline {subsection}{\numberline {5.2.2}Recursive wighted least squares}{21}{}%
\contentsline {subsubsection}{RWLS I}{21}{}%
\contentsline {chapter}{\numberline {6}Prediction error methods}{23}{}%
\contentsline {subsection}{\numberline {6.0.1}Identification of ARMAX models}{25}{}%
\contentsline {subsection}{\numberline {6.0.2}Statistical properties of PEM estimators}{25}{}%
\contentsline {subsection}{\numberline {6.0.3}MISO ARX models}{26}{}%
\contentsline {chapter}{\numberline {7}Statistical hypothesis testing}{27}{}%
\contentsline {chapter}{\numberline {8}Model complexity selection and regularization}{29}{}%
\contentsline {subsection}{\numberline {8.0.1}The F-test}{29}{}%
\contentsline {subsection}{\numberline {8.0.2}The final prediction error (FPE) criterion}{30}{}%
\contentsline {subsection}{\numberline {8.0.3}Criteria with complexity terms}{31}{}%
\contentsline {subsection}{\numberline {8.0.4}Akaike information criterien (AIC)}{31}{}%
\contentsline {subsubsection}{Minimum description lenght (MDL) criterion}{31}{}%
\contentsline {chapter}{\numberline {9}model assesment (validation)}{33}{}%
\contentsline {section}{\numberline {9.1}Whiteness test}{33}{}%
\contentsline {section}{\numberline {9.2}Test of cross-correlation}{34}{}%
\contentsline {chapter}{\numberline {10}Maximum likelihood estimation}{35}{}%
\contentsline {section}{\numberline {10.1}the gaussian case}{35}{}%
\contentsline {subsection}{\numberline {10.1.1}the Cram√©r-Rao lower bound}{36}{}%
\contentsline {chapter}{\numberline {11}Classification: probabilistic models}{37}{}%
\contentsline {section}{\numberline {11.1}The Bayes classifier}{37}{}%
\contentsline {section}{\numberline {11.2}Logistic regression}{38}{}%
\contentsline {subsubsection}{How to classify a new input $u(t)$?}{39}{}%
\contentsline {subsection}{\numberline {11.2.1}The gradient descent algorithm}{39}{}%
\contentsline {subsubsection}{Stochastic gradient descent}{40}{}%
\contentsline {subsubsection}{Confusion matrix}{40}{}%
\contentsline {subsection}{\numberline {11.2.2}Multiclass problems}{40}{}%
\contentsline {subsection}{\numberline {11.2.3}Dealing with nonlinear boundaries}{41}{}%
\contentsline {subsection}{\numberline {11.2.4}Linear discriminant analysis}{41}{}%
\contentsline {subsection}{\numberline {11.2.5}Gaussian class densities with commmon covariance matrix}{42}{}%
\contentsline {subsubsection}{Multiclass problem}{42}{}%
\contentsline {subsection}{\numberline {11.2.6}Gaussian class denisities with different covariance matrices}{42}{}%
\contentsline {chapter}{\numberline {12}Classification: deterministic models}{43}{}%
\contentsline {subsection}{\numberline {12.0.1}Separating hyperplanes}{44}{}%
\contentsline {subsection}{\numberline {12.0.2}the maximum margin classifier}{44}{}%
\contentsline {subsubsection}{Constrained optimization problems: the Lagrange multipliers}{44}{}%
\contentsline {subsubsection}{Finding the maximum margin classifier by using the Lagrange multipliers}{46}{}%
\contentsline {section}{\numberline {12.1}Support vector machine}{47}{}%
\contentsline {subsection}{\numberline {12.1.1}Dealing with nonlinear boundaries: the kernel trick}{49}{}%
\contentsline {subsubsection}{How does it work?}{49}{}%
\contentsline {subsection}{\numberline {12.1.2}Multiclass problems}{49}{}%
\contentsline {subsubsection}{One-vs-all approach}{49}{}%
\contentsline {subsubsection}{One-vs-one approach}{50}{}%
\contentsline {chapter}{\numberline {13}Regularization}{51}{}%
\contentsline {section}{\numberline {13.1}Regularized least squares: the Ridge regression}{51}{}%
