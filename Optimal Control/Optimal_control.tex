
\documentclass{book}
\usepackage{amsmath, amsthm, graphicx, amsfonts, float}
\usepackage[english]{babel}
\graphicspath{ {./images/} }

\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,237mm},
 left=20mm,
 top=30mm,
 }
 \usepackage[hidelinks]{hyperref}

\newcommand\at[2]{\left.#1\right|_{#2}}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\des}{des}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\notimplies}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

\title{Industrial Robotics M}
\author{Dante Piotto}
\date{spring semester 2023}


\begin{document}

\chapter{Introduction to optimal control}

\section{Optimal control problem formulation}
Consider the continuous-time system ($t\in\R$)

\begin{gather}
    \dot{x}(t) = f(x(t),u(t),t) \\
    y(t) = h(x(t),u(t),t)
\end{gather}
\begin{itemize}
    \item $x(t)\in\R^n$ state of the system at time $t$ 
    \item $u(t)\in\R^m$ input of the system at time $t$ 
    \item $y(t)\in\R^p$ output of the system at time $t$
\end{itemize}
We will mainly work with time invariant systems, $\dot{x}(t)=f(x(t),u(t))$. 

We consider nonlinear, discrete-time systems described by 
\[
    x(t+1)=f_t(x(t),u(t)) \quad t\in\N_0
\]
but from now on we will use the compact notation
\[
    x_{t+1}=f_t(x_t,u_t) \quad t\in\N_0
\]
where $x_t\in\R^n$ and $u_t\in\R^m$ are the state and the input of the system at time $t$.

Consider a nonlinear, discrete-time system on a finite time horizon 
\[
    x_{t+1} = f_t(x_t,u_t) \quad t=0,\dots,T-1
\]
We use $\mathbf{x}\in\R^{nT}$ and $\mathbf{u}\in\R^{mT}$ to denote, respectively, the stack of the states $x_t$ for all $t\in\{1,\dots,T\}$ and the unputs $u_t$ for all $t\in\{0,\dots,T-1\}$, that is:
\begin{gather*}
    \mathbf{x} := \begin{bmatrix}
        x_1 \\ \vdots \\ x_T
    \end{bmatrix} \qquad
    \mathbf{u} := \begin{bmatrix}
        u_0 \\ \vdots \\ u_{T-1}
    \end{bmatrix}
\end{gather*}

\subsubsection{Trajectory of a system}

Definition: A pair ($\bar{\mathbf{x}},\bar{\mathbf{u}})\in\R^{nT} \times \R^{mT}$ is called a trajectory of system (1) % TODO fix
if $\bar{x}_{t+1}=f_t(\bar{x}_t,\bar{u}_t)$ for all $t\in\{0,\dots,T-1\}$., That is, if $\bar{\mathbf{x}},\bar{\mathbf{u}})$ satisfies the system dynamics (the same holds for continuous time systems with proper adjustments). In particular, $\bar{\mathbf{x}}$ is the state trajectory, while $\bar{\mathbf{u}}$ is the input trajectory.

\subsubsection{Equilibrium}
Definition: A state-input pair $(x_e,u_e)\in\R^n\times\R^m$ is called an equilibrium pair of (1) % TODO fix
if $(x_t,u_t)=(x_e,u_e),t\in\N_0$ is a trajectory of the system. \\
Equilibria of time-invariant systems satisfy $x_e=f(x_e,u_e)$

\subsubsection{Linearization of a system about a trajectory}
Given the dynamics (1)% TODO fix 
and a trajectory $(\bar{\mathbf{x}},\bar{\mathbf{u}})$, the linearization of (1) about $(\bar{\mathbf{x}},\bar{\mathbf{u}})$ is given by the linear (possibly) time-varying system 
\[
    \Delta x_{t+1} = A_t\Delta x_t + B_t \Delta u_t \quad t\in\N_0
\]
with $A_t$ and $B_t$ the Jacobians of $f_t$, with respect to state and input respectively, evaluated at $(\bar{\mathbf{x}},\bar{\mathbf{u}})$
\[
A_t = \at{\displaystyle\frac{\partial}{\partial x}f(\bar{x}_t,\bar{u}_t)}{(\bar{\mathbf{x}},\bar{\mathbf{u}})} \quad B_t = \at{\displaystyle\frac{\partial}{\partial u}f(\bar{x}_t,\bar{u}_t)}{(\bar{\mathbf{x}},\bar{\mathbf{u}})}
\]

\subsection{Optimization}
\subsubsection{Main ingredients}
\begin{itemize}
    \item Decision variable: $x\in\R^n$ 
    \item Cost function: $\ell(x):\R^n\to\R$ cost associated to decision $x$
        \item Constraints (constraint sets): for some given functions $h_i:\R^n\to\R, i=1,\dots,m, \text{ and } g_j:\R^n\to\R,$ the decision vector $x\in\R^n$ needs to satisfy 
            \begin{gather*}
                h_i(x)=0 \quad i=1,\dots,m \\
                g_j(x)=0 \quad j=1,\dots,r
            \end{gather*}
        equivalently we can say that we require $x\in X$ with 
        \[
            X=\{x\in\R^n|h(x)=0, g(x)\leq 0\},
        \]
        where we compactly denoted $h(x)=\col(h_1(x),\dots,h_m(x))$ and $g(x) = \col(g_1(x),\dots,g_r(x))$
\end{itemize}

\subsubsection{Minimization}
We can write our optimization problem as 
\begin{gather}
    \min_{x\in\R^n} \ell(x)\\
    \text{subj. to } h_i(x) = 0 \quad i=1,\dots,m\\
    g_j(x)\leq 0 \quad j=1,\dots,r
\end{gather}
where $h_i:\R^n\to\R$ and $g_j:\R^n\to\R$\\
We can write it more compactly as 
\begin{gather*}
    \min_{x\in\R^n} \ell(x)\\
    \text{subj. to } h(x) = 0 
    g(x)\leq 0 
\end{gather*}
where $h:\R^n\to\R^m$ and $g:\R^n\to\R^r$
\subsection{Discrete-time optimal control}
\subsubsection{main ingredients}
\begin{itemize}
    \item Dynamics: a discrete-time system in state space form 
        \[
            x_{t+1} = f_t(x_t,u_t) \quad t=0,1,\dots,T-1
        \]
    \item the dynamics introduce $T$ equality constraints 
        \[
            \begin{array}{l l l}
                x_1 = f(x_0,u_0) & \quad \text{i.e.} \quad& x_1-f_t(x_0,u_0)=0 \\
                x_2 = f(x_1,u_1) & \quad \text{i.e.} \quad & x_1-f_t(x_1,u_1)=0 \\
                \vdots \\
                x_T = f(x_{T-1},u_{T-1}) & \quad \text{i.e.} \quad & x_T-f_t(x_{T-1},u_{T-1})=0 \\
            \end{array}
        \] 
        This is equivalent to $nT$ scalar constraints
    \item Cost function: a cost "to be payed" for a chosen trajectory. We consider an additive structure in time 
        \[
            \ell(\mathbf{x},\mathbf{u}) = \displaystyle\sum_{t=0}^{T-1}\ell_t(x_t,u_t)+\ell_T(x_T)
        \]
        where $\ell_t:\R^n\times\R^m\to\R$ is called stage-cost, while $\ell_T:\R^n\to\R$ is the terminal cost. 
    \item End-point constraints: function of the state variable prescribed at initial and/or final point 
        \[
            r(x_0,x_T)=0
        \]
    \item Path constraints: point-wise (in time) constraints representing possible limits on states and inputs at each time $t$ 
        \[
            g_t(x_t,u_t)\leq 0, \quad t\in\{0,\dots,T-1\}
        \]
\end{itemize}

A discrete-time optimal control problem can be written as 
\begin{gather*}
    \min_{\substack{x_0,x_1,\dots,x_T\\u_0,\dots,u_{T-1}}} \displaystyle\sum_{t=0}^{T-1}\ell_t(x_t,u_t)+\ell_T(x_T)\\
    \begin{array}{l l }
        \text{subj. to } & x_{t+1}=f_t(x_t,u_t), \quad t\in\{0,\dots,T-1\}\\
                         & r(x_0,x_T) = 0  \\
                         & g_t(x_t,u_t)\leq 0, \quad t\in\{0,\dots,T-1\}
    \end{array}
\end{gather*}

\subsubsection{Optimal control for trajectory generation}
We can pose a trajectory generation problem as 
\begin{gather*}
    \min_{\mathbf{x}\in\R^n,\mathbf{u}\in\R^m}\displaystyle\sum_{t=0}^{T-1}\displaystyle\frac{1}{2}\|x_t-x_t^{\des}\|^2_Q + \displaystyle\frac{1}{2}\|u_t-u_t^{\des}\|^2_R + \displaystyle\frac{1}{2}\|x_T-x_T^{\des}\|^2_{P_f}
\end{gather*}

\subsubsection{Continuous-time Optimal Control problem}
A continuous-time optimal control problem, i.e., $t\in\R$ can be written as 
\begin{gather*}
    \min_{(x(\cdot),u(\cdot))\in \mathcal{F}}\displaystyle\int_{0}^{T}\ell_\tau(x(\tau),u(\tau))d\tau+\ell_T(x(T))\\
    \begin{array}{l l}
        \text{subj. to } & \dot{x}(t) = f_t(x(t),u(t)) \quad t\in[0,T]\\
                         & r(x(0),x(T)) = 0 \\
                         & g_t(x(t),u(t))\leq 0 \quad t\in[0,T)
    \end{array}
\end{gather*}
Note that $\mathcal{F}$ is a space of functions (function space). This is an infinite dimensional optimization problem
\begin{itemize}
    \item Cost functional $\ell:\mathcal{F}\to\R$
        \[
            \ell(x(\cdot),u(\cdot)) = \displaystyle\int_{0}^{T}\ell_\tau(x(\tau),u(\tau))d\tau+\ell_T(x(T))
        \]
    \item Space of trajectories ( or trajectory manifold)
        \[
            \mathcal{T} = \{(x(\cdot),u(\cdot))\in\mathcal{F}|\dot{x}(t)=f_t(x(t),u(t)), t\geq 0\}
        \]
\end{itemize}

\chapter{Nonlinear Optimization}
\section{Unconstrained Optimization}
Consider the unconstrained optimization problem 
\[
    \min_{x\in\R^n}\ell(x)
\]
with $\ell:\R^n\to\R$ a cost function to be minimized and $x$ a decision vector 

We say that $x^*$ is a  
\begin{itemize}
    \item global minimum if $\ell(x^*)\leq\ell(x)$ for all $x\in\R^n$ 
        \item strict global minimum if $\ell(x^*)<\ell(x)$ for all $x\neq x^*$
        \item local minimum if there exists $\epsilon>0$ such that $\ell(x^*)\leq\ell(x)$ for all $x\in B(x^*,\epsilon) = \{x\in\R^n| \|x-x^*\|<\epsilon\}$
            \item strict local minimum if there exists $\epsilon>0$ such that $\ell(x^*)<\ell(x)$ for all $x\in B(x^*,\epsilon)$ 

\end{itemize}
\subsubsection{Notation}
We denote $\ell(x^*)$ the optimal (minimum) value of a generic optimization problem, i.e. 
\[
    \ell(x^*) = \min_{x\in\R^n}\ell(x)
\]
where $x^*$ is the minimum point (optimal value for the optimization variable) i.e. 
\[
    x^* = \argmin_{x\in\R^n} \ell(x)
\]
\subsubsection{Gradient and Hessian}
Gradient of a function: for a function $r:\R^n\to\R$  the gradient is denoted as 
\[
    \nabla r(x) = \begin{bmatrix}
         \displaystyle\frac{\partial r(x)}{\partial x_1} \\ \vdots \\ \displaystyle\frac{\partial r(x)}{\partial x_n}
     \end{bmatrix} \in \R^{n\times 1}
\]
Hessian matrix of a function: for a fcuntion $r:\R^n\to\R$  the Hessian matrix is denoted as 
\[
    \nabla^2(r(x)) = \begin{bmatrix}
        \displaystyle\frac{\partial^2 r(x)}{\partial x_1^2} & \cdots & \displaystyle\frac{\partial^2 r(x)}{\partial x_1x_n} \\ \vdots & \ddots & \vdots \\ \displaystyle\frac{\partial^2 r(x)}{\partial x_nx_1} & \cdots & \displaystyle\frac{\partial^2 r(x)}{\partial x_1^2}
    \end{bmatrix}
\]
Gradient of a vector-valued function: for a vector field $r:\R^n\to\R^m$, the gradient is denoted as 
\[
    \nabla r(x) = \begin{bmatrix}
        \nabla r_1(x) & \cdots & \nabla r_m(x)
    \end{bmatrix} = \begin{bmatrix}
        \displaystyle\frac{\partial r_1(x)}{\partial x_1} & \cdots & \displaystyle\frac{\partial r_m(x)}{\partial x_1} \\
        \vdots & \ddots & \vdots \\
        \displaystyle\frac{\partial r_1(x)}{\partial x_n} & \cdots & \displaystyle\frac{\partial r_m(x)}{\partial x_n} \\
    \end{bmatrix} \in \R^{n\times m}
\]
which is the transpose of the Jacobian matrix of $r$
\subsection{Conditions of optimality}
\subsubsection{First order necessary condition (FNC) of optimality (unconstrained)}
Let $x^*$ be an unconstrained local minimum of $\ell:\R^n\to\R$ and assume that $\ell$ is continuously differentiable ($\mathcal{C}^1$) in $B(x^*,\epsilon)$ for some $\epsilon>0$. Then $\nabla \ell(x^*)=0$
\subsubsection{Second order necessary condition (FNC) of optimality (unconstrained)}
If additionally $\ell$ is twice continuously differentiable ($\mathcal{C}^2$) in $B(x^*,\epsilon)$, then $\nabla^2 \ell(x^*)\geq 0$ (The Hessian of $\ell$ is positive semidifinite)
\subsubsection{Second order sufficient conditions of optimality (unconstrained)}
Let $\ell:\R^n\to\R\in\mathcal{C}^2$ in $b(x^*,\epsilon)$ for some $\epsilon>0$. Suppose that $x^*\in\R^n$ satisfies 
\[
    \nabla\ell(x^*) = 0 and \nabla^2\ell(x^*)>0
\]
Then $x^*$ is a strict (unconstrained) local minimum of $\ell$
\subsubsection{Convex set} 
A set $X\subset \R^n$ is convex if for any two points $x_A$ and $x_B$ in $X$ and for all $\lambda\in[0,1]$, then 
\[
    \lambda x_a+(1-\lambda)x_B \in X
\]
\subsubsection{Convex functions}
Let $X \subset \R^n$ be a convex set. A function $\ell:X\to\R$ is convex if for any two points $x_A$ and $x_B$ in $X$ and for all $\lambda\in[0,1]$, then 
\[
    \ell(\lambda x_A + (1-\lambda)x_B)\leq \lambda\ell(x_A)+ (1-\lambda)\ell(x_B)
\]
% TODO slide 12-13 
\subsection{Minimization of convex functions}
\subsubsection{Proposition}
Let $X\subset\R^n$ be a convex set and $\ell: X\to\R$ a convex function. Then a local minimum of $\ell$ is also a global minimum \\
Proof: not done in class but present in slides for funsies
\subsubsection{Necessary and sufficient condition of optimality (unconstrained)}
For the unconstrained minimization of a convex function it can be shown that the first order necessary condition of optimality is also sufficient (for a global minimum).
\subsubsection{Proposition}
Let $\ell_\R^n \to \R$ be a convex function. Then $x^*$ is a global minimum if and only if $\nabla\ell(x^*)=0$

Proof: not done in class but present in slides for funsies

\subsection{Quadratic programming}
Let us consider a special class of optimization problems, namely quadratic optimization problems or quadratic programs: 
\[
    \min_{x\in\R^n}x^TQx+b^tx
\]
with $Q=Q^T\in\R^{n\times n}$ and $b\in\R^n$
\subsubsection{optimality conditions}
First-order necessary condition for optimality: if $x^*$ is a minimum then 
\[
    \nabla \ell(x^*)=0 \implies 2Qx^*+b=0
\]
Second-order necessary condition for optimality: if $x^*$ is a minimum then 
\[
    \nabla^2\ell(x^*)\geq 0 \implies 2Q>0
\]
A necessary condition for the existence of minima for a quadratic program is that $Q\geq 0$. Thus, quadratic programs admitting at least a minimum are convex optimization problems.
\subsubsection{properties}
Since quadratic programs are convex programs ($Q\geq 0$ is necessary to have a local minimum), then the following holds: 
\begin{itemize}
    \item For a quadratic program necessary conditions of optimality are also sufficient and minima are global
\end{itemize}
If $Q>0$, then there exists a unique global minimum given by 
\[
    x^* = -\displaystyle\frac{1}{2}Q^{-1}b
\]

\section{Unconstrained Optimization Algorithms}
\subsection{Iterative descent methods}
We consider optimization algorithms relying on the iterative descent idea. We denote $x^k\in\R^n$ an estimate of a local minimum at iteration $k\in\N$. The algorithm starts at a given initial guess $x^0$ and iteratively generates vectors $x^1,x^2,\dots$ such that $\ell$ is decreased at each iteration, i.e. 
\[
    \ell(x^{k+1})<\ell(x^k) \qquad k = 1,2,\dots
\]
\subsubsection{two-step procedure}
We consider a general two-step procedure that reads as follows 
\[
    x^{k+1} = x^k+\gamma^k d^k, \qquad k=1,2,\dots
\]
in which 
\begin{enumerate}
    \item each $\gamma^k>0$ is a "step-size" 
    \item $d^k\in\R^n$ is a "direction"
\end{enumerate}
The goal is to 
\begin{enumerate}
    \item choose a direction $d^k$ along which the cost decreases for $\gamma^k$ sufficiently small;
        \item select a step-size $\gamma^k$ guaranteeing a sufficient decrease. 
\end{enumerate}
In oher references these are called line-search methods.
\subsection{Gradient methods}
Let $x^k$ be such that $\nabla\ell(x^k)\neq 0$. We start by considering the update rule 
\[
    x^{k+1} = x^k-\gamma^k\nabla\ell(x^k)
\]
i.e., we choose $d^k = \nabla\ell(x^k)$

From the first order Taylor expansion of $\ell$ at $x$ we have 
\begin{gather*}
    \begin{array}{r c l}
        \ell(x^{k+1}) & = & \ell(x^k)+\nabla\ell(x^k)^T(x^{k+1}-x^k)+o(\|x^{k+1}-x^k\|)\\
                      & = & \ell(x^k)-\gamma^k\|\nabla\ell(x^k)\|^2+o(\gamma^k)
     \end{array}
\end{gather*}
Thus, for $\gamma^k>0$ sufficiently small it can be shown that $\ell(x^k+1)<\ell(x^k)$

The update rule 
\[
    x^{k+1}=x^k-\gamma^k\nabla\ell(x^k)
\]
can be generalized to so called \emph{gradient methods}
\[
    x^{k+1}=x^k+\gamma^kd^k
\]
with $d^k$ such that
\[
    \nabla\ell(x^k)^Td^k<0
\]
Also, $d^k$ must be gradient related, i.e. $d^k$ must not asymptotically become perpendicular to $\nabla\ell$
\subsubsection{selecting the descent direction}
Several gradient methods can be written as 
\[
    x^{k+1} 0 x^k-\gamma^kD^k\nabla\ell(x^k) \quad k=1,2,\dots
\]
where $D^k\in\R^{n\times n}$ is  a symmetric positive definite matrix. It can be immediately seen that 
\[
    -\nabla\ell(x^k)^TD^k\nabla\ell(x^k)<0
\]
i.e. $d^k = -D^k\nabla\ell(x^k)$ is a descent direction. The choice of $D^k$ must be made such that there exist $d_1,d_2$ positive real, such that $d_1 I \leq D^k \leq d_2 I$

Some choices for $D^k$:
\begin{itemize}
    \item Steepest descent $D^k=I_n$
    \item Newton's method $D^k = (\nabla^2\ell(x^k))^{-1}$\\
        It can be used when $\nabla^2\ell(x^k)>0$. It typically converges very fast asymptotically. For $\gamma^k = 1$ pure Newton's method
    \item Discretized Newton's method $D^k=(H(x^k))^{-1}$, where $H(x^k)$ is a positive definite symmetric approximation of $\nabla^2\ell(x^k)$ obtained by using finite difference approximations of the second derivatives 
    \item Some regularized version of the Hessian
\end{itemize}
\subsection{gradient method}
The update rule obtained for $D^k=I$ is called steepest descent. The name steepest descent is due to the following property: the normalized negative gradient direction 
\[
    d^k = -\displaystyle\frac{\nabla\ell(x^k)}{\|\nabla\ell(x^k)\|}
\]
minimizes the slope $\nabla \ell(x^k)^Td^k$ among all normalized directions, i.e. it gives the steepest descent.

\subsection{Newton's method for root finding}
Consider the nonlinear root finding problem 
\[
    r(x) = 0
\]
Idea: iteratively refine the solution such that the improved guess $x^{k+1}$ represents a root of the linear approximation of $r$ about the current tentative solution $x^k$. Consider the linear approximation of $r$ about $x^k$, we have 
\[
    r^k(x^k+\Delta x^k) = r(x^k)+\nabla r(x^k)^T\Delta x^k
\]
then, finding the zeros of the approximation, we have
\[
    \Delta x^k = -(\nabla r(x^k)^T)^{-1}r(x^k)
\]
Thus, the solution is improved as 
\[
    x^{k+1} = x^k-(\nabla r(x^k)^T)^{-1}r(x^k)
\]
\subsection{Newton's method for unconstrained optimization}
Consider the unconstrained optimization problem 
\[
    \min_{x\in\R^n} \ell(x)
\]
stationary points $\bar{x}$ satisfy the first order optimality condition 
\[
    \nabla \ell (\bar{x}) = 0
\]
We can look at it as a root finding problem, with $r(x)=\nabla\ell(x)$, and solve it via Newton's method. Therefore, we can compute $\Delta x^k$ as the solution of the linearization of $r(x)=\nabla\ell(x)$ at $x^k$, i.e. 
\[
    \nabla \ell(x^k) + \nabla^2\ell(x^k)\Delta x^k = 0
\]
and run the update 
\[
    x^{k+1} = x^k -(\nabla^2\ell(x^k))^{-1}\nabla\ell(x^k)
\]
We can introduce a variable step-size 
\[
    x^{k+1} = x^k-\gamma^k(\nabla^2\ell(x^k))^{-1}\nabla\ell(x^k)
\]
This is called generalized Newton's method
\subsubsection{Newton's method via Quadratic Optimization}
Observe that 
\[
    \nabla\ell(x^k) +\nabla^2\ell(x^k)\Delta x^k = 0
\]
is the first-order necessary and sufficient condition of optimality for the quadratic program 
\begin{equation}
    \label{qp}
    \Delta x^k = \argmin_{\Delta x}\nabla\ell(x^k)^T \Delta x+\displaystyle\frac{1}{2}\Delta x^T\nabla^2\ell(x^k)\Delta x
\end{equation}
Thus, the $k$-th iteration of Newton's method can be seen as 
\[
    x^{k+1} = x^k+\Delta x^k
\]
with $\Delta x^k$ solution of the quadratic problem \ref{qp}. Generalized version: 
\[
    x^{k+1} = x^k + \gamma^k \Delta x^k
\]

\subsection{Gradient methods via quadratic optimization}
Similarly to Newton's method, a descent direction $\Delta x^k=D^k\nabla\ell(x^k)$ can be seen as the direction that minimizes at each iteration a different quadratic approximation of $\ell$ about $x^k$. In fact, consider the quadratic approximation $\ell^k(x)$ about $x^k$ given by 
\[
    \ell^k(x) = \ell(x^k)+\nabla\ell(x^k)^T(x-x^k)+\displaystyle\frac{1}{2}(x-x^k)^T(D^k)^{-1}(x-x^k)
\]
By setting the derivative to zero, we have 
\[
    \nabla\ell(x^k)+(D^k)^{-1}(x-x^k)=0
\]
we can calculate the minimum of $\ell^k(x)$ and set it as the next iterate $x^{k+1}$
\[
    \Delta x^k = -D^k\nabla\ell(x^k)
\]
\subsection{step-size selection rules}
\begin{itemize}
    \item Constant step-size: $\gamma^k=\gamma>0$
        \item Diminishing step-size: $\gamma^k\to 0$ as $k\to\infty$. It must hold that \[
                \displaystyle\sum_{k=0}^{\infty}\gamma^k = +\infty \quad \text{and} \quad \displaystyle\sum_{k=0}^{\infty}(\gamma^k)^2 < +\infty
        \]
        The above conditions avoid pathological choices of $\gamma^k$
        \item minimization rule
        \item Armijo rule
\end{itemize}
\subsubsection{Armijo rule}
Step-size is selected following the procedure: 
\begin{enumerate}
    \item set $\bar{\gamma}^0>0,\quad \beta\in(0,1),\quad c\in(0,1)$
\end{enumerate}
given $d^k$ descent direction we can consider 
\[
    g(\gamma) = \ell(x^k+\gamma d^k), \quad g:\R\to\R
\]
% TODO insert image slide 30
The value of $g(\gamma)$ for $\gamma=0$ is $\ell(x^k)$. The minimization rule chooses as the value for $\gamma$ the value that minimizes $g(\gamma)$. The partial minimization rule would search for a minimum in a restricted set of values for $\gamma$























\chapter{Optimality conditions for optimal control}





\chapter{Linear Quadratic (LQ) optimal control}


\chapter{Dynamic Programming}



\chapter{Numerical methods for nonlinear optimal control}




\chapter{Optimization-based predictive control}



































\end{document}
